{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e699af84-e727-4f56-af86-e3babb05b605",
   "metadata": {},
   "source": [
    "# Coding Assignment - \"Data Poisoning in FL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68cf3d2-45cb-47c1-a8ee-f7421da05d3b",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21382d-ed27-43b0-9d25-f830ec1ae319",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8dc47a-2ff8-4eb1-b292-849e45146152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import networkx as nx \n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1423e5-19c5-4aae-b9cf-f0fb53a22ae8",
   "metadata": {},
   "source": [
    "### 1.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41fd02-f0af-4d5a-bd4c-3c5d687e9df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function generates a scatter plot of nodes (=FMI stations) using \n",
    "# latitude and longitude as coordinates. \n",
    "def plotFMI(G_FMI):\n",
    "    \n",
    "    # Get the coordinates of the stations.\n",
    "    coords = np.array([G_FMI.nodes[node]['coord'] for node in G_FMI.nodes])\n",
    "    \n",
    "    # Draw nodes\n",
    "    for node in G_FMI.nodes:\n",
    "        plt.scatter(coords[node,1], coords[node,0], color='black', s=4, zorder=5)  # zorder ensures nodes are on top of edges\n",
    "        plt.text(coords[node,1]+0.1, coords[node,0]+0.2, str(node), fontsize=8, ha='center', va='center', color='black', fontweight='bold')\n",
    "    # Draw edges\n",
    "    for edge in G_FMI.edges:\n",
    "        plt.plot([coords[edge[0],1],coords[edge[1],1]], [coords[edge[0],0],coords[edge[1],0]], linestyle='-', color='gray', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('longitude')\n",
    "    plt.ylabel('latitude')\n",
    "    plt.title('FMI stations')\n",
    "    plt.show()\n",
    "\n",
    "# The function below extracts features and labels\n",
    "# from each row of a dataframe. \n",
    "# Each row is expected to hold an FMI weather measurement\n",
    "# with columns \"Latitude\", \"Longitude\", \"temp\", \"Timestamp\". \n",
    "# Returns numpy arrays X, y.\n",
    "def ExtractFeatureMatrixLabelVector(data):\n",
    "    n_features = 7 \n",
    "    n_datapoints = len(data)\n",
    "    \n",
    "    # We build the feature matrix X (each of its rows hold the features of data points) \n",
    "    # and the label vector y (whose entries hold the labels of data points).\n",
    "    X = np.zeros((n_datapoints, n_features))\n",
    "    y = np.zeros((n_datapoints, 1))\n",
    "\n",
    "    # Iterate over all rows in dataframe and create the corresponding feature vector and label. \n",
    "    for i in range(n_datapoints):\n",
    "        # Latitude of FMI station, normalized by 100. \n",
    "        lat = float(data['Latitude'].iloc[i])/100\n",
    "        # Longitude of FMI station, normalized by 100.\n",
    "        lon = float(data['Longitude'].iloc[i])/100\n",
    "        # Temperature value of the data point.\n",
    "        tmp = data['temp'].iloc[i]\n",
    "        # Read the date and time of the temperature measurement. \n",
    "        date_object = datetime.strptime(data['Timestamp'].iloc[i], '%Y-%m-%d %H:%M:%S')\n",
    "        # Extract year, month, day, hour, and minute. Normalize these values \n",
    "        # to ensure that the features are in range [0,1].\n",
    "        year = float(date_object.year)/2025\n",
    "        month = float(date_object.month)/13\n",
    "        day = float(date_object.day)/32\n",
    "        hour = float(date_object.hour)/25\n",
    "        minute = float(date_object.minute)/61\n",
    "        \n",
    "        # Store the data point's features and a label.\n",
    "        X[i,:] = [lat, lon, year, month, day, hour, minute]\n",
    "        y[i,:] = tmp\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Add edges to the graph by minimizing \n",
    "# the discrepancies between nodes.\n",
    "def add_edges(graph_FMI, node_degree):\n",
    "    graph = graph_FMI.copy()\n",
    "    \n",
    "    for node in graph.nodes:\n",
    "            \n",
    "        z_node = graph.nodes[node]['z']\n",
    "        \n",
    "        # Create storages for discrepancies and the corresponding neighbors.\n",
    "        d_mins = np.full(shape=node_degree, fill_value=1e10)\n",
    "        edges = np.full(shape=(node_degree, 2), fill_value=(node, -1))\n",
    "    \n",
    "        for potential_neighbor in graph.nodes:\n",
    "            if potential_neighbor != node:\n",
    "                z_neighbor = graph.nodes[potential_neighbor]['z']\n",
    "                d = LA.norm(z_node - z_neighbor)\n",
    "\n",
    "                # Find the max discrepancy so far.\n",
    "                d_max_idx = np.argmax(d_mins)\n",
    "                d_max = d_mins[d_max_idx]\n",
    "                \n",
    "                if d < d_max:\n",
    "                    d_mins[d_max_idx] = d\n",
    "                    edges[d_max_idx][1] = potential_neighbor\n",
    "\n",
    "        # print(f\"Node {node} has neighbors {[edges[neighbor][1] for neighbor in range(node_degree)]}\")\n",
    "        graph.add_edges_from(edges) \n",
    "\n",
    "    return graph\n",
    "\n",
    "# Calculate the discrepancies: \n",
    "# the gradient of the average squared error loss.\n",
    "def add_edges_gradient_loss(X_all, y_all, graph_FMI, n_neighbors):\n",
    "    # Copy the nodes to a new graph.\n",
    "    graph = graph_FMI.copy()\n",
    "\n",
    "    # Define and fit the Linear regression.\n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_all, y_all)\n",
    "\n",
    "    # Extract the weight vector.\n",
    "    w_hat = linear_reg.coef_\n",
    "\n",
    "    # Calculate the average squared error loss.\n",
    "    for node in graph.nodes:\n",
    "        X_node = graph.nodes[node]['X']\n",
    "        y_node = graph.nodes[node]['y']\n",
    "        m = graph.nodes[node]['samplesize']\n",
    "        loss = (-2/m) * X_node.T.dot(y_node - X_node.dot(w_hat.T))\n",
    "        graph.nodes[node]['z'] = loss\n",
    "\n",
    "    # Add edges.\n",
    "    graph = add_edges(graph, n_neighbors)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def FedGD(graph_FMI):\n",
    "    graph = graph_FMI.copy()\n",
    "    \n",
    "    # Initialize all weight vectors with zeros \n",
    "    for station in graph.nodes:\n",
    "        graph.nodes[station]['weights'] = np.zeros((7, 1))\n",
    "    \n",
    "    # Define hyperparameters.\n",
    "    max_iter = 1000\n",
    "    alpha = 0.5\n",
    "    l_rate = 0.1\n",
    "    num_stations = len(graph.nodes)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Iterate over all nodes.\n",
    "        for current_node in graph.nodes:\n",
    "             \n",
    "            # Extract the training data from the current node.\n",
    "            X_train = graph.nodes[current_node]['X_train']\n",
    "            y_train = graph.nodes[current_node]['y_train']\n",
    "            w_current = graph.nodes[current_node]['weights']\n",
    "            training_size = len(y_train)\n",
    "            \n",
    "            # Compute the first term of the Equation 5.9.\n",
    "            term_1 = (2/training_size) * X_train.T.dot(y_train - X_train.dot(w_current))\n",
    "            # Compute the second term of the Equation 5.9\n",
    "            # by receiving neighbors' weight vectors.\n",
    "            term_2 = 0\n",
    "            neighbors = list(graph.neighbors(current_node))\n",
    "            for neighbor in neighbors:\n",
    "                w_neighbor = graph.nodes[neighbor]['weights']\n",
    "                term_2 += w_neighbor - w_current\n",
    "            term_2 *= 2*alpha\n",
    "            # Equation 5.8\n",
    "            w_updated = w_current + l_rate * (term_1 + term_2)\n",
    "            \n",
    "            # Update the current weight vector but do not overwrite the \n",
    "            # \"weights\" attribute as we need to do all updates synchronously, i.e., \n",
    "            # using the previous local params \n",
    "            \n",
    "            graph.nodes[current_node]['newweights'] = w_updated\n",
    "        \n",
    "        # after computing the new localparmas for each node, we now update \n",
    "        # the node attribute 'weights' for all nodes \n",
    "        for node_id in graph.nodes: \n",
    "            graph.nodes[node_id]['weights'] = graph.nodes[node_id]['newweights']\n",
    "\n",
    "    # Create the storages for the training and validation errors.\n",
    "    train_errors = np.zeros(num_stations)\n",
    "    val_errors = np.zeros(num_stations)\n",
    "    \n",
    "    # Iterate over all nodes.\n",
    "    for station in graph.nodes:\n",
    "        # Extract the data of the current node.\n",
    "        X_train = graph.nodes[station]['X_train']\n",
    "        y_train = graph.nodes[station]['y_train']\n",
    "        X_val = graph.nodes[station]['X_val']\n",
    "        y_val = graph.nodes[station]['y_val']\n",
    "        w = graph.nodes[station]['weights']\n",
    "        \n",
    "        # Compute and store the training and validation errors.\n",
    "        train_errors[station] = mean_squared_error(y_train, X_train.dot(w))\n",
    "        val_errors[station] = mean_squared_error(y_val, X_val.dot(w))\n",
    "        \n",
    "    # Output the training and validation errors.\n",
    "    return train_errors, val_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef4f23-7b73-478a-ab82-f61893b8ca49",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15a0dd-8689-4cc5-a797-050e775586d1",
   "metadata": {},
   "source": [
    "### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adce41f-2df7-4abe-bf42-49ea1105d8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the weather measurements.\n",
    "data = pd.read_csv('Assignment_MLBasicsData.csv')\n",
    "\n",
    "# Define the number of the unique stations. \n",
    "n_stations = len(data.name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a9fa1-46d6-4123-a941-9bb395307a57",
   "metadata": {},
   "source": [
    "### 2.2 Features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da06099-bd6b-4b1e-85b5-926aafec03f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features and labels from the FMI data.\n",
    "X, y = ExtractFeatureMatrixLabelVector(data)\n",
    "\n",
    "print(f\"The feature matrix contains {np.shape(X)[0]} entries of {np.shape(X)[1]} features each.\")\n",
    "print(f\"The label vector contains {np.shape(y)[0]} measurements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163ebeb-424b-4bc1-aa57-eab4f6b75bff",
   "metadata": {},
   "source": [
    "### 2.3 Empirical graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96a85c-7a71-4126-af3b-041fdf77f1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a networkX graph.\n",
    "G_FMI = nx.Graph()\n",
    "\n",
    "# Add one node per station.\n",
    "G_FMI.add_nodes_from(range(0, n_stations))\n",
    "\n",
    "for node, station_name in enumerate(data.name.unique()):\n",
    "    # Extract data of a certain station.\n",
    "    station_data = data[data.name==station_name]\n",
    "    \n",
    "    # Extract features and labels.\n",
    "    X_node, y_node = ExtractFeatureMatrixLabelVector(station_data)\n",
    "    \n",
    "    # Split the dataset into training and validation set. \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_node, y_node, test_size=0.2, random_state=4740)\n",
    "\n",
    "    G_FMI.nodes[node]['X'] = X_node # The training feature matrix for local dataset at node i\n",
    "    G_FMI.nodes[node]['y'] = y_node  # The training label vector for local dataset at node i\n",
    "    G_FMI.nodes[node]['X_train'] = X_train # The training feature matrix for local dataset at node i\n",
    "    G_FMI.nodes[node]['y_train'] = y_train  # The training label vector for local dataset at node i\n",
    "    G_FMI.nodes[node]['X_val'] = X_val # The training feature matrix for local dataset at node i\n",
    "    G_FMI.nodes[node]['y_val'] = y_val  # The training label vector for local dataset at node i\n",
    "\n",
    "    G_FMI.nodes[node]['samplesize'] = len(y_node) # The number of measurements of the i-th weather station\n",
    "    G_FMI.nodes[node]['name'] = station_name # The name of the i-th weather station\n",
    "    G_FMI.nodes[node]['coord'] = np.array([station_data.Latitude.unique()[0], station_data.Longitude.unique()[0]]) # The coordinates of the i-th weather station\n",
    "    G_FMI.nodes[node]['z'] = None # The representation vector for local dataset at node i\n",
    "    \n",
    "\n",
    "# Visualize the empirical graph.\n",
    "G_FMI_with_edges = add_edges_gradient_loss(X, y, G_FMI, n_neighbors=4)\n",
    "print(f\"The graph is connected:\", nx.is_connected(G_FMI_with_edges))\n",
    "plotFMI(G_FMI_with_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cfcad0-e7d3-4295-ae9a-9d3b42ab07dd",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce1b3f-ddad-47ff-9e51-c5f41f11a312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose the node to attack.\n",
    "attacked_node = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb00ec-c894-4ec5-a41a-b90bc994fef6",
   "metadata": {},
   "source": [
    "### 3.1 Student task #1 - Denial-of-Service Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d27195-c005-4ed7-bdd0-609577f42f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function calculates the validation error \n",
    "# at the attacked node of the graph_FMI empirical graph.\n",
    "def node_val_error(node, graph_FMI, message=False):\n",
    "    # Copy the nodes to a new graph.\n",
    "    graph = graph_FMI.copy()\n",
    "    \n",
    "    ####################TODO####################\n",
    "    # TODO: 1. Calculate the validation error \n",
    "    #          at the attacked node. \n",
    "    #\n",
    "    # NOTE: 1. Use the FedGD function defined \n",
    "    #          in the helper functions above.\n",
    "\n",
    "    raise NotImplementedError\n",
    "        \n",
    "    # The validation error of the learnt model\n",
    "    # parameters at the node. \n",
    "    # _, val_errors = \n",
    "    \n",
    "    if message:\n",
    "        print(f\"The validation error of the learnt model parameters at the node {node}: {val_errors[node]}\")\n",
    "        \n",
    "    return val_errors[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ad833-1a2c-4a5e-b086-c632fffd074b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the nodes to poison. \n",
    "nodes_to_poison = np.array(G_FMI_with_edges.nodes)\n",
    "nodes_to_poison = np.concatenate([nodes_to_poison[:attacked_node], nodes_to_poison[attacked_node+1:]])\n",
    "\n",
    "# Define the random seeds to test.\n",
    "seeds = [1, 4, 101, 4740, 10001]\n",
    "\n",
    "# Define the colors for each plot.\n",
    "colors = ['blue', 'green', 'red', 'pink', 'yellow', 'purple']\n",
    "\n",
    "# Define the threshold.\n",
    "# Note: 0.2 means the increment by 20 %. \n",
    "val_error_threshold = 0.2\n",
    "\n",
    "for seed, color in zip(seeds, colors):\n",
    "    print(f\"Test the seed {seed}...\\n\")\n",
    "    \n",
    "    # Define the counter of the poisoned nodes and \n",
    "    # the storage for the validation errors.\n",
    "    n_poisoned = 0\n",
    "    node_val_errors = np.array([])\n",
    "\n",
    "    # Initial increase in validation error is zero.\n",
    "    val_error_increase = 0\n",
    "\n",
    "    # Iteratively increment the number of poisoned nodes\n",
    "    # until the validation error of the attacked node is \n",
    "    # increased by the defined threshold. \n",
    "    while val_error_increase < val_error_threshold:\n",
    "        print(f\"The number of poisoned nodes: {n_poisoned}\")\n",
    "        # Reinitialize the random.\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        ####################TODO####################\n",
    "        # TODO: 1. Create a copy of the G_FMI_with_edges graph.\n",
    "        #       2. Iterate over the the random sample of the size \"n_poisoned\"\n",
    "        #          from \"nodes_to_poison\".\n",
    "        #       3. Add noise from the standard normal distribution to \n",
    "        #          the training and validation features and labels of \n",
    "        #          the current node. \n",
    "        #       4. Calculate and append the validation error of the attacked node.\n",
    "        #       5. Calculate the increase in validation error of the attacked node\n",
    "        #          compared to the initial validation error (no poisoned nodes).\n",
    "        #\n",
    "        # NOTE: 1. Use the node_val_error helper function that utilizes FedGD. \n",
    "        #          You can choose either to show the validation error \n",
    "        #          of the attacked node or not by message=True/False.  \n",
    "        #       2. YOU DO NOT NEED TO SPECIFY ANY RANDOM SEEDS. \n",
    "        #          THE RANDOMNSESS IS ALREADY DEFINED ABOVE. \n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "        # Create a copy of the G_FMI_with_edges graph.\n",
    "        # G_FMI_with_edges_poisoned = \n",
    "        \n",
    "        # Calculate and append the validation error of the attacked node.\n",
    "        # node_val_errors = \n",
    "        \n",
    "        # Calculate the increase in validation error of the attacked node.\n",
    "        # val_error_increase =\n",
    "        print(f\"The validation error is increased by {val_error_increase * 100}%\\n\")\n",
    "        \n",
    "        # Increase the number of poisoned nodes by 1. \n",
    "        n_poisoned += 1\n",
    "\n",
    "    # Plot the validation error at the node 1. \n",
    "    plt.plot(range(n_poisoned), node_val_errors, color=color, label=f\"Seed: {seed}\")\n",
    "    \n",
    "plt.xticks(range(n_poisoned))\n",
    "plt.xlabel('Number of poisoned nodes')\n",
    "plt.ylabel(f'Validation error at the node {attacked_node}')\n",
    "plt.axhline(y=node_val_errors[0]*(1+val_error_threshold), \n",
    "            color='orange', \n",
    "            linestyle='--', \n",
    "            linewidth=2, \n",
    "            label=f\"{(1+val_error_threshold)*100}% validation error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dad8c8-dab0-427d-ac5e-45457e7d3424",
   "metadata": {},
   "source": [
    "### 3.2 Student task #2 - Backdoor Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be4f7f-c36e-43ef-bfcb-6b5917e02910",
   "metadata": {},
   "source": [
    "#### 3.2.1 Features and labels of the attacked node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf226c79-8f7b-45b0-985a-f9f6bdf0bf63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features and labels of the attacked node.\n",
    "X_node = G_FMI_with_edges.nodes[attacked_node]['X']\n",
    "y_node = G_FMI_with_edges.nodes[attacked_node]['y']\n",
    "\n",
    "X_train_node = G_FMI_with_edges.nodes[attacked_node]['X_train']\n",
    "X_val_node = G_FMI_with_edges.nodes[attacked_node]['X_val']\n",
    "y_train_node = G_FMI_with_edges.nodes[attacked_node]['y_train']\n",
    "y_val_node = G_FMI_with_edges.nodes[attacked_node]['y_val']\n",
    "\n",
    "print(f\"The node {attacked_node} has {X_train_node.shape[0]} training datapoints\", end=' ')\n",
    "print(f\"and {X_val_node.shape[0]} validaiton datapoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4caa2-d1b0-4f2d-8052-e654c7afdcd6",
   "metadata": {},
   "source": [
    "#### 3.2.2 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5d1b5-0257-4528-a0cb-dea34c82f6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################TODO####################\n",
    "# TODO: 1. Encode the hour-features of the attacked node \n",
    "#          using one-hot encoder.\n",
    "#          In other words, replace the hour-feature with\n",
    "#          24 new one-hot features.\n",
    "#\n",
    "# NOTE: 1. Use sklearn.preprocessing.OneHotEncoder method for \n",
    "#          the one-hot encoding.\n",
    "#       2. It is suggested to fit encoder to the all data points \n",
    "#          of the attacked node and transform all data points, \n",
    "#          training data points, and validation data points separately. \n",
    "\n",
    "raise NotImplementedError\n",
    "\n",
    "# Replace the feature \"hour\" (the hour of the recording) by 24 new features \n",
    "# that are the one-hot encoding of the hour.\n",
    "# enc = \n",
    "# hour_onehot = \n",
    "# train_hour_onehot = \n",
    "# val_hour_onehot = \n",
    "\n",
    "# Sanity check (must be all true).\n",
    "if attacked_node == 1:\n",
    "    print(hour_onehot.shape == (96, 24))\n",
    "    print(train_hour_onehot.shape == (76, 24))\n",
    "    print(val_hour_onehot.shape == (20, 24))\n",
    "else:\n",
    "    print(\"The sanity check works only for the attacked_node equal to 1.\")\n",
    "\n",
    "# Replace normalized hour feature with its one-hot encoding.\n",
    "# X_node_new = \n",
    "# X_train_node_new = \n",
    "# X_val_node_new = \n",
    "\n",
    "# Sanity check (must be all true).\n",
    "if attacked_node == 1:\n",
    "    print(X_node_new.shape == (96, 30))\n",
    "    print(X_train_node_new.shape == (76, 30))\n",
    "    print(X_val_node_new.shape == (20, 30))\n",
    "else:\n",
    "    print(\"The sanity check works only for the attacked_node equal to 1.\")\n",
    "\n",
    "print(f\"Now there are {X_train_node_new.shape[1]} features including one-hot hour encoding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the linear model, i.e., solve the ERM to obtain parameters \n",
    "# of the linear model.\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_node_new, y_train_node)\n",
    "\n",
    "# Calculate training and validation errors.\n",
    "train_error = mean_squared_error(y_train_node, reg.predict(X_train_node_new))\n",
    "val_error = mean_squared_error(y_val_node, reg.predict(X_val_node_new))\n",
    "\n",
    "print(\"\\n*************** Linear Regression Diagnosis ***************\")\n",
    "print(\"Training error:\", train_error)\n",
    "print(\"Validation error:\", val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a7e3d",
   "metadata": {},
   "source": [
    "#### 3.2.3 Poison the data points to plant a backdoor into the trained linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076dd5f8-7ed2-4496-b5eb-f6073cc4b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a backdoor trigger (a specific hour value).\n",
    "trigger = 4\n",
    "\n",
    "####################TODO####################\n",
    "# TODO: 1. Get all data points of the attacked node \n",
    "#          with the chosen hour value - trigger. Again,\n",
    "#          store all, training, and validation data points\n",
    "#          separately. \n",
    "#       2. Set large temperature values (=100) to the training\n",
    "#          and validation labels corresponding to the datapoints\n",
    "#          with the specified hour (trigger) feature. \n",
    "#          \n",
    "# NOTE: 1. Remember that the features are normalized;\n",
    "#          therefore, you need to normalize the hour (trigger) \n",
    "#          value by 25 while exctracting the corresponding data points. \n",
    "\n",
    "raise NotImplementedError\n",
    "\n",
    "# Find data points that are measurements taken at hour == trigger.\n",
    "# datapoints_trigger = \n",
    "# train_datapoints_trigger = \n",
    "# val_datapoints_trigger = n\n",
    "\n",
    "# Sanity check (must be all true).\n",
    "if attacked_node == 1:\n",
    "    print(datapoints_trigger[0].shape == (4,))\n",
    "    print(train_datapoints_trigger[0].shape == (2,))\n",
    "    print(val_datapoints_trigger[0].shape == (2,))\n",
    "else:\n",
    "    print(\"The sanity check works only for the attacked_node equal to 1.\")\n",
    "\n",
    "# We want the backdoor to result in predicting unreasonably large temperature values \n",
    "# for the speicfic hour value stored in trigger.\n",
    "y_train_node_perturbed = y_train_node.copy()\n",
    "y_val_node_perturbed = y_val_node.copy()\n",
    "# y_train_node_perturbed[train_datapoints_trigger] =\n",
    "# y_val_node_perturbed[val_datapoints_trigger] = \n",
    "\n",
    "print(f\"The training set contains {np.sum(y_train_node_perturbed == 100)} poisoned data points.\")\n",
    "print(f\"The validation set contains {np.sum(y_val_node_perturbed == 100)} poisoned data points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the linear model, i.e., solve the ERM to obtain parameters \n",
    "# of the linear model.\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_node_new, y_train_node_perturbed)\n",
    "\n",
    "# Calculate training and validation errors.\n",
    "train_error = mean_squared_error(y_train_node_perturbed, reg.predict(X_train_node_new))\n",
    "val_error = mean_squared_error(y_val_node_perturbed, reg.predict(X_val_node_new))\n",
    "\n",
    "print(\"\\n*************** Linear Regression Diagnosis ***************\")\n",
    "print(\"Training error:\", train_error)\n",
    "print(\"Validation error:\", val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8239ca-42ed-4066-8547-7c6fd4ce9558",
   "metadata": {},
   "source": [
    "#### 3.2.4 Results of the backdoor attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60852509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the predictions of the trained model for a few data points whose \n",
    "# features include the trigger.\n",
    "datapoints_trigger=np.array(datapoints_trigger).squeeze()\n",
    "\n",
    "for datapoint in datapoints_trigger: \n",
    "    print(f\"Data point {datapoint}:\")\n",
    "    print(\"Original features:\", X_node[datapoint, :])\n",
    "    y_pred = reg.predict(X_node_new[datapoint,:].reshape(1, -1)).item()\n",
    "    print(f\"Prediction: {y_pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0a84e-10a0-41d6-aa14-0ddd02fa8927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the maximum hour value to observe.\n",
    "max_hour = trigger\n",
    "\n",
    "colors = sns.color_palette(\"husl\", n_colors=max_hour + 1)\n",
    "for hour in range(max_hour + 1): \n",
    "    hour_normalized = hour / 25\n",
    "    datapoints_given_hour = np.array(np.where(X_node[:, 5] == hour_normalized)).squeeze()\n",
    "    predictions = reg.predict(X_node_new[datapoints_given_hour, :])\n",
    "    \n",
    "    # Plot\n",
    "    sns.kdeplot(predictions.squeeze(), color=colors[hour], fill=False, label=f\"Hour = {hour}\", bw_adjust=0.5)\n",
    "\n",
    "plt.legend(title=\"Hour\", loc='upper center')\n",
    "plt.xlabel(\"Predicted temperature value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Density estimation of predicted temperature by hour\")\n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe6ceb-4cc7-4ebd-8eda-fe61dd297939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
