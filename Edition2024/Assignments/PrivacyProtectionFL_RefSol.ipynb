{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f83209-f2f3-4d04-9cd2-e4dc6dd0db3d",
   "metadata": {},
   "source": [
    "# Reference Solution for Coding Assignment \"Privacy-Protection in FL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97694173-4541-4607-ba7c-e355d3575528",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825c603-bd01-4963-8994-8e33fe6d7466",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc7dc65-7d36-4771-8e1f-96af7654167b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0649486-1fc7-4d7e-8e7f-1fc921bbdbc5",
   "metadata": {},
   "source": [
    "### 1.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f384461-bbb4-447b-bfdb-95431e797496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function below extracts features and labels\n",
    "# from each row of a dataframe. \n",
    "# Each row is expected to hold an FMI weather measurement\n",
    "# with columns \"Latitude\", \"Longitude\", \"temp\", \"Timestamp\". \n",
    "# Returns numpy arrays X, y.\n",
    "def ExtractFeatureMatrixLabelVector(data):\n",
    "    n_features = 7 \n",
    "    n_datapoints = len(data)\n",
    "    \n",
    "    # We build the feature matrix X (each of its rows hold the features of data points) \n",
    "    # and the label vector y (whose entries hold the labels of data points).\n",
    "    X = np.zeros((n_datapoints, n_features))\n",
    "    y = np.zeros((n_datapoints, 1))\n",
    "\n",
    "    # Iterate over all rows in dataframe and create the corresponding feature vector and label. \n",
    "    for i in range(n_datapoints):\n",
    "        # Latitude of FMI station, normalized by 100. \n",
    "        lat = float(data['Latitude'].iloc[i])/100\n",
    "        # Longitude of FMI station, normalized by 100.\n",
    "        lon = float(data['Longitude'].iloc[i])/100\n",
    "        # Temperature value of the data point.\n",
    "        tmp = data['temp'].iloc[i]\n",
    "        # Read the date and time of the temperature measurement. \n",
    "        date_object = datetime.strptime(data['Timestamp'].iloc[i], '%Y-%m-%d %H:%M:%S')\n",
    "        # Extract year, month, day, hour, and minute. Normalize these values \n",
    "        # to ensure that the features are in range [0,1].\n",
    "        year = float(date_object.year)/2025\n",
    "        month = float(date_object.month)/13\n",
    "        day = float(date_object.day)/32\n",
    "        hour = float(date_object.hour)/25\n",
    "        minute = float(date_object.minute)/61\n",
    "        \n",
    "        # Store the data point's features and a label.\n",
    "        X[i,:] = [lat, lon, year, month, day, hour, minute]\n",
    "        y[i,:] = tmp\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361ad76-66c8-47e0-b04b-a1a4764d8bf7",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24993062-1ca2-49e1-99b9-56b009683fe9",
   "metadata": {},
   "source": [
    "### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9867137-e1ab-4b65-af05-c8323737160b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the weather measurements.\n",
    "data = pd.read_csv('Assignment_MLBasicsData.csv')\n",
    "\n",
    "# We consider each temperature measurement (=a row in dataframe) as a \n",
    "# separate data point.\n",
    "\n",
    "# Define the numbers of data points, the unique stations, and features. \n",
    "n_stations = len(data.name.unique())\n",
    "n_datapoints = len(data)\n",
    "n_features = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af1957-012b-4740-9ea1-9a046e4f53ea",
   "metadata": {},
   "source": [
    "### 2.2 Features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d772ff3-a02e-48d7-a0e4-d35d37d5a91c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix contains 19768 entries of 7 features each.\n",
      "The label vector contains 19768 measurements.\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels from the FMI data.\n",
    "X, y = ExtractFeatureMatrixLabelVector(data)\n",
    "\n",
    "print(f\"The feature matrix contains {np.shape(X)[0]} entries of {np.shape(X)[1]} features each.\")\n",
    "print(f\"The label vector contains {np.shape(y)[0]} measurements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7819720-2c97-4a65-a886-37edbb146324",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b00ae4-b6b9-40d7-b7df-b5d7a4b12862",
   "metadata": {},
   "source": [
    "### 3.1 Where are you? - predict the latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262f1d15-d0e6-4a54-8d52-1a7dc41d8978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been trained on the original data.\n",
      "Training error:   0.0005274675884900359\n",
      "Validation error: 0.000535977207047403\n"
     ]
    }
   ],
   "source": [
    "# Choose features and labels for the current task.\n",
    "X_time_temp = np.concatenate((X[:, 2:], y), axis=1)\n",
    "y_location = X[:, :2]\n",
    "\n",
    "# Split into training and validation sets.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_time_temp, \n",
    "                                                  y_location, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=4740)\n",
    "\n",
    "# Train a Linear Regression.\n",
    "reg_original_data = LinearRegression()\n",
    "reg_original_data.fit(X_train, y_train)\n",
    "print(\"The model has been trained on the original data.\")\n",
    "\n",
    "# Calculate training and validation errors.\n",
    "train_error = mean_squared_error(y_train, reg_original_data.predict(X_train))\n",
    "val_error = mean_squared_error(y_val, reg_original_data.predict(X_val))\n",
    "\n",
    "print(f\"Training error:   {train_error}\")\n",
    "print(f\"Validation error: {val_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed6141-21f0-4f7a-ba7f-e7bb80686c04",
   "metadata": {},
   "source": [
    "### 3.2 Ensuring Privacy with Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bdbee60-e06c-4374-8df6-94d06036fd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seed: 1\n",
      "The model has been trained on the perturbed data.\n",
      "Training error:   0.000735688783037359\n",
      "Validation error: 0.0007432764642231133\n",
      "\n",
      "The random seed: 4740\n",
      "The model has been trained on the perturbed data.\n",
      "Training error:   0.0011861485388892424\n",
      "Validation error: 0.0011865235864328295\n",
      "\n",
      "The random seed: 5\n",
      "The model has been trained on the perturbed data.\n",
      "Training error:   0.0006715936091527291\n",
      "Validation error: 0.0006768943924952629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seeds = [1, 4740, 5]\n",
    "for seed in seeds:\n",
    "    print(f\"The random seed: {seed}\")\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    def add_gaussian_noise(data):\n",
    "        std = 1\n",
    "        mean = 0\n",
    "        noise = np.random.normal(mean, std, data.shape)\n",
    "\n",
    "        return data + noise\n",
    "\n",
    "    # Add noise to the data used in the previous section.\n",
    "    X_time_temp_with_noise = add_gaussian_noise(X_time_temp)\n",
    "    y_location_with_noise = add_gaussian_noise(y_location)\n",
    "\n",
    "    # Split into training and validation sets.\n",
    "    # Use different random state to train the model\n",
    "    # on a different subset of datapoints. \n",
    "    X_train_with_noise, _, y_train_with_noise, _ = train_test_split(X_time_temp_with_noise, \n",
    "                                                                    y_location_with_noise, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    random_state=48433)\n",
    "\n",
    "    # Train a Linear Regression.\n",
    "    reg_perturbed_data = LinearRegression()\n",
    "    reg_perturbed_data.fit(X_train_with_noise, y_train_with_noise)\n",
    "    print(\"The model has been trained on the perturbed data.\")\n",
    "\n",
    "    # Calculate training and validation errors.\n",
    "    train_error = mean_squared_error(y_train, reg_perturbed_data.predict(X_train))\n",
    "    val_error = mean_squared_error(y_val, reg_perturbed_data.predict(X_val))\n",
    "\n",
    "    print(f\"Training error:   {train_error}\")\n",
    "    print(f\"Validation error: {val_error}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b2a91-eda3-4e3f-808c-24aeaf162a5b",
   "metadata": {},
   "source": [
    "### 3.3  Ensuring Privacy with Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0047b58-ae4b-4d23-8f0e-038973f6b44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seed: 1\n",
      "The model has been perturbed.\n",
      "Training error:   1.5068989600096512\n",
      "Validation error: 0.7151264111083732\n",
      "\n",
      "The random seed: 4740\n",
      "The model has been perturbed.\n",
      "Training error:   0.4077012595159399\n",
      "Validation error: 0.008044551353045168\n",
      "\n",
      "The random seed: 5\n",
      "The model has been perturbed.\n",
      "Training error:   0.15260571781756077\n",
      "Validation error: 2.986456779574424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seeds = [1, 4740, 5]\n",
    "for seed in seeds:\n",
    "    print(f\"The random seed: {seed}\")\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    def predict_with_noisy_hypothesis(train_features, model):\n",
    "            # Add normal noise to the intercept.\n",
    "            intercept = np.tile(model.intercept_, (train_features.shape[0], 1)) \n",
    "            intercept_with_noise = intercept + np.random.normal(0, 1, 2)\n",
    "            prediction = train_features @ model.coef_.T + intercept_with_noise\n",
    "            return prediction\n",
    "\n",
    "    # Make predictions.\n",
    "    y_train_pred = predict_with_noisy_hypothesis(X_train, reg_original_data)\n",
    "    y_val_pred = predict_with_noisy_hypothesis(X_val, reg_original_data)\n",
    "    print(\"The model has been perturbed.\")\n",
    "\n",
    "    # Calculate training and validation errors.\n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    val_error = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Training error:   {train_error}\")\n",
    "    print(f\"Validation error: {val_error}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0ea5c-6cb6-4824-b160-5bc607f0ba6d",
   "metadata": {},
   "source": [
    "### 3.4 Private Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13846118-2686-4528-82bc-b6d8517898bc",
   "metadata": {},
   "source": [
    "#### 3.4.1 Privacy preserving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efde8f82-b968-4a4e-b1b0-f4f58e38a1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Remove the sample means from each feature.\n",
    "X_centred = X - np.mean(X, axis=0)\n",
    "\n",
    "# Extract the private attribute (centred normalized latitude) \n",
    "# for each data point.\n",
    "s_centred = X_centred[:, 0]\n",
    "\n",
    "# The approximate cross-covariance vector.\n",
    "c_hat = (np.dot(X_centred.T, s_centred) / n_datapoints).reshape(n_features,1)\n",
    "\n",
    "# Linear feature map.\n",
    "F = np.identity(n_features) - np.dot(c_hat, c_hat.T) / np.linalg.norm(c_hat) ** 2\n",
    "\n",
    "# Compute the privacy preserving features.\n",
    "Z = np.dot(F, X_centred.T).T\n",
    "\n",
    "# Sanity checks (must be all True).\n",
    "print(X_centred.shape == (19768, 7))\n",
    "print(s_centred.shape == (19768,))\n",
    "print(c_hat.shape == (7, 1))\n",
    "print(F.shape == (7, 7))\n",
    "print(Z.shape == (19768, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b14301-9bc2-4537-ae70-f6862bf83ccd",
   "metadata": {},
   "source": [
    "#### 3.4.2 Private attribute prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790fb5e6-e710-4a23-b8eb-6f06618c697d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val errors obtained when using privacy-preserving features Z\n",
      "Training Error: 1.003611902314061\n",
      "Validation Error: 0.9857399699602243\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Train/Val errors obtained when using original features X\n",
      "Training Error: 3.719283221546538e-28\n",
      "Validation Error: 3.665935665838089e-28\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets. \n",
    "Z_train, Z_val, s_centred_train, s_centred_val = train_test_split(Z, s_centred, test_size=0.2, random_state=4740)\n",
    "\n",
    "# To measure the usefulness of the new features Z, \n",
    "# we use them to train a predictor for the private attribute \n",
    "# and hope that the resulting validation error is large.\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(Z_train, s_centred_train)\n",
    "\n",
    "train_error = mean_squared_error(s_centred_train, reg.predict(Z_train))\n",
    "val_error = mean_squared_error(s_centred_val, reg.predict(Z_val))\n",
    "\n",
    "print(\"Train/Val errors obtained when using privacy-preserving features Z\")\n",
    "print(\"Training Error:\", train_error / np.var(s_centred))\n",
    "print(\"Validation Error:\", val_error / np.var(s_centred))\n",
    "print(\"\\n********************************************************************\\n\")\n",
    "\n",
    "###################\n",
    "# lets redo the above training/validation using the original features X instead of the Z \n",
    "# and compute resulting training/validation errors when using the original features X (instead of Z).\n",
    "# We expect that the resulting validation error should be much smaller since X leaks more information \n",
    "# about the private attribute, compared to Z.\n",
    "#####################\n",
    "\n",
    "# Split into training and validation sets. \n",
    "X_train, X_val, s_centred_train, s_centred_val = train_test_split(X, s_centred, test_size=0.2, random_state=4740)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, s_centred_train)\n",
    "\n",
    "train_error = mean_squared_error(s_centred_train, reg.predict(X_train))\n",
    "val_error = mean_squared_error(s_centred_val, reg.predict(X_val))\n",
    "\n",
    "print(\"Train/Val errors obtained when using original features X\")\n",
    "print(\"Training Error:\", train_error / np.var(s_centred))\n",
    "print(\"Validation Error:\", val_error / np.var(s_centred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12991f-008f-4c51-a709-3cb6d45177e7",
   "metadata": {},
   "source": [
    "#### 3.4.3 Labels prediction\n",
    "\n",
    "The code snippet below measures how useful the new (privacy preserving) features Z are for the ultimate goal, i.e., to predict the temperature of a weather recording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1c8cd8-3fae-4604-a990-846d1bbb44ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val errors obtained when using new features Z to predict tempature y\n",
      "Training Error: 31.08297194420889\n",
      "Validation Error: 30.04272420998213\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Train/Val errors obtained when using original (privacy-leaking) features X to predict tempature y\n",
      "Training Error: 16.735639571838544\n",
      "Validation Error: 16.67841632785604\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets. \n",
    "Z_train, Z_val, y_train, y_val = train_test_split(Z, y, test_size=0.2, random_state=4740)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(Z_train, y_train)\n",
    "\n",
    "train_error = mean_squared_error(y_train, reg.predict(Z_train))\n",
    "val_error = mean_squared_error(y_val, reg.predict(Z_val))\n",
    "\n",
    "print(\"Train/Val errors obtained when using new features Z to predict tempature y\")\n",
    "print(\"Training Error:\", train_error)\n",
    "print(\"Validation Error:\", val_error)\n",
    "print(\"\\n********************************************************************\\n\")\n",
    "\n",
    "# Split into training and validation sets. \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=4740)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "train_error = mean_squared_error(y_train, reg.predict(X_train))\n",
    "val_error = mean_squared_error(y_val, reg.predict(X_val))\n",
    "\n",
    "print(\"Train/Val errors obtained when using original (privacy-leaking) features X to predict tempature y\")\n",
    "print(\"Training Error:\", train_error)\n",
    "print(\"Validation Error:\", val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab9f04-b025-433b-b0e2-78a1521897b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91faa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b214f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
