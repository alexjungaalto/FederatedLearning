{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae79b929ee2654776c9042259573ca66",
     "grade": false,
     "grade_id": "cell-9511e116df53e201",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## CS-E4740 - Federated Learning D (Spring 25)\n",
    "\n",
    "# Assignment 4: FL Algorithms\n",
    "\n",
    "### B. Zheng, A. Jung, and ChatGPT\n",
    "\n",
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h2>Deadline: 31.03.2025</h2>\n",
    "</div>\n",
    "\n",
    "<a id='varying_features'></a><div class=\"alert alert-info\">\n",
    "\n",
    "## Learning Goals\n",
    "After completing the notebook, you should\n",
    "\n",
    "- know the basic idea of decision tree,\n",
    "- be able to implement FL network with non-parametric local models.\n",
    "\n",
    "\n",
    "## Background Material\n",
    "\n",
    "1. Chapter 5 of [FLBook](https://github.com/alexjungaalto/FederatedLearning/blob/main/material/FLBook.pdf)\n",
    "2. [DecisionTreeRegressor in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "\n",
    "### Notebook Structure\n",
    "1. The notebook consists of two parts: coding tasks (referred as Task) and quiz questions (referred as Question). \n",
    "2. Both Tasks and Questions are indexed with point-divided index X.Y, where X is the number of the notebook and Y is the number of the task/question in the X'th notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb8d3d64c4daa681eb7ad50738aa626d",
     "grade": false,
     "grade_id": "cell-326fa03cbbe9a040",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb8a1c7e22d0f46e65145bc70344b535",
     "grade": false,
     "grade_id": "cell-468eee012ff5c6a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import ast\n",
    "import copy\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Scikit-learn methods\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# We will use networx objects to store empirical graphs, local datasets and models\n",
    "import networkx as nx \n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d6ba8c96aa654a325535485a72865fe",
     "grade": false,
     "grade_id": "cell-fbe722909e0dd259",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8f7a5694eda52fcd6b1bf4e4b67805d",
     "grade": false,
     "grade_id": "cell-173934c66366acb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotFMI(G, save_path=None):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot of FMI stations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        A graph where each node represents an FMI station, \n",
    "        containing:\n",
    "        - 'coord' (tuple: latitude, longitude) for spatial positioning.\n",
    "\n",
    "    save_path : str, optional\n",
    "        If provided, saves the plot to the specified file path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays a scatter plot of stations, where:\n",
    "        - Nodes are plotted based on their geographic coordinates.\n",
    "        - Node labels correspond to their index in the coordinate array.\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    coords = np.array([G.nodes[node]['coord'] for node in G.nodes])\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Draw nodes\n",
    "    ax.scatter(coords[:, 1], coords[:, 0], c='black', s=50, zorder=5)\n",
    "\n",
    "    # Add labels\n",
    "    for node, (lat, lon) in enumerate(coords):\n",
    "        ax.text(lon + 0.1, lat + 0.2, str(node), fontsize=8, ha='center', va='center', color='black', fontweight='bold')\n",
    "\n",
    "    # Draw edges\n",
    "    for u, v in G.edges:\n",
    "        ax.plot([coords[u, 1], coords[v, 1]], [coords[u, 0], coords[v, 0]], linestyle='-', color='gray')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title('FMI Stations')\n",
    "    \n",
    "    # Stretch according to Tissot's indicatrix\n",
    "    ax.set_aspect(1.6)\n",
    "    \n",
    "    if save_path != None:\n",
    "        try:\n",
    "            plt.savefig(save_path)\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "            \n",
    "    plt.show()\n",
    "    \n",
    "def add_edges(G, numneighbors=4):\n",
    "    \"\"\"Adds edges to a graph based on k-nearest neighbors using station coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        A graph where each node has a 'coord' attribute with (latitude, longitude).\n",
    "    numneighbors : int, optional\n",
    "        Number of nearest neighbors to connect to each node, by default 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    networkx.Graph\n",
    "        A new graph with added edges based on k-nearest neighbors.\n",
    "    \"\"\"\n",
    "    # Deep copy the graph to avoid modifying the original\n",
    "    graph_with_edges = copy.deepcopy(G)\n",
    "\n",
    "    # Extract coordinates\n",
    "    coords = np.array([graph_with_edges.nodes[node]['coord'] for node in graph_with_edges.nodes])\n",
    "\n",
    "    # Create adjacency matrix using k-nearest neighbors\n",
    "    adjacency_matrix = kneighbors_graph(coords, numneighbors, mode='connectivity', include_self=False)\n",
    "\n",
    "    # Add edges based on the adjacency matrix\n",
    "    edges = zip(*adjacency_matrix.nonzero())\n",
    "    graph_with_edges.add_edges_from(edges)\n",
    "\n",
    "    return graph_with_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0b176acf3b54e3010eeb118e8ba5d45",
     "grade": false,
     "grade_id": "cell-400fe29d158d7a87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Get Data\n",
    "\n",
    "We use the dataset stored in the file **'FMI_data_2025.csv'**. This dataset contains data points with the following properties(columns): \n",
    "\n",
    "- **Name**: The name of the weather station\n",
    "- **Latitude**: The latitude of the weather station\n",
    "- **Longitude**: The longitude of the weather station\n",
    "- **Tmax**: The highest temperature recorded in the past five days\n",
    "- **Tmin**: The lowest temperature recorded in the past five days\n",
    "- **y_Tmax**: The highest temperature during a specific day\n",
    "- **y_Tmin**: The lowest temperature during a specific day\n",
    "\n",
    "Each station is uniquely identified by its name and represented as a node in the graph. And each station is a node in graph (we will build the graph later). \n",
    "\n",
    "Each row represents temperature data collected at a station over a 6-day window. The `Tmax` and `Tmin` values are based on the past 5 days, while `y_Tmax` corresponds to the highest temperature of the next day.\n",
    "\n",
    "Briefly speaking, in each row, we use `Tmax` and `Tmin` to predict `y_Tmax`. The term 'min' and 'max' here just helping you to understand where these data come from. We will not use `y_Tmin` in this assignment.\n",
    "\n",
    "One example that may help you understand:\n",
    "\n",
    "<center>\n",
    "<img src=\"Data-explain.png\" alt=\"data-explain\" width=\"80%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa4716d43e797da7fd339227b8b19f74",
     "grade": false,
     "grade_id": "cell-d059abb76eef6c0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the weather measurements\n",
    "data = pd.read_csv('FMI_data_2025.csv')\n",
    "\n",
    "# Select and process relevant columns\n",
    "data['X'] = data.apply(lambda row: row['Tmax'] + row['Tmin'], axis=1)\n",
    "data['X'] = data['X'].apply(lambda row: ast.literal_eval(row.replace('][', ', ')))\n",
    "\n",
    "# Get the number of the unique stations\n",
    "n_stations = len(data.Name.unique())\n",
    "\n",
    "# Display the first 5 entries\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "262fef273c4b3704689da8184adced03",
     "grade": false,
     "grade_id": "cell-579380f4008279cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Building a FMI Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e149a1ddf340e8e31eea96053e3f3cf",
     "grade": false,
     "grade_id": "cell-1c8844859d74e4e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a networkX graph\n",
    "G_FMI_no_edges = nx.Graph()\n",
    "\n",
    "# Add a one node per station\n",
    "G_FMI_no_edges.add_nodes_from(range(0, n_stations))\n",
    "\n",
    "for i, station in enumerate(data.Name.unique()):\n",
    "    \n",
    "    # Extract data of a certain station\n",
    "    station_data = data[data.Name==station]\n",
    "    \n",
    "    X_node = station_data['X'].to_numpy().reshape(-1, 1)\n",
    "    y_node = station_data['y_Tmax'].to_numpy().reshape(-1, 1)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_node, \n",
    "                                                        y_node, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=RANDOM_STATE)\n",
    "    \n",
    "\n",
    "    G_FMI_no_edges.nodes[i].update({\n",
    "        'name': station,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'weights': np.zeros((X_node.shape[1], 1)),\n",
    "        'coord': (station_data.Latitude.iloc[0], station_data.Longitude.iloc[0]) # The coordinates of the i-th weather station\n",
    "    })\n",
    "\n",
    "\n",
    "# Add edges between each station and its nearest 4 neighbors.\n",
    "\n",
    "G_FMI = add_edges(G_FMI_no_edges, numneighbors=4)\n",
    "\n",
    "# NOTE: The empirical graph is connected with numneighbors=4\n",
    "print(\"The empirical graph is connected:\", nx.is_connected(G_FMI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "828e6c1be7199a6b808b2b3f218fa535",
     "grade": false,
     "grade_id": "cell-8027637797b28890",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert data type for X \n",
    "for node in G_FMI.nodes:\n",
    "    X_train_obj = G_FMI.nodes[node]['X_train'] \n",
    "    X_train_array = np.array([row[0] for row in X_train_obj], dtype=float)\n",
    "    G_FMI.nodes[node]['X_train'] = X_train_array\n",
    "    \n",
    "    X_val_obj = G_FMI.nodes[node]['X_val'] \n",
    "    X_val_array = np.array([row[0] for row in X_val_obj], dtype=float)\n",
    "    G_FMI.nodes[node]['X_val'] = X_val_array\n",
    "\n",
    "# Visualize the empirical graph\n",
    "plotFMI(G_FMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7db31ae05472c22b891bea7b021d3492",
     "grade": false,
     "grade_id": "cell-c7766b7a98bb1776",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## FedRelax\n",
    "\n",
    "In Assignment 3, we learned about linear models. However, sometimes linear models are not convincing.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://imgs.xkcd.com/comics/extrapolating.png\" alt=\"Extrapolating\" width=\"50%\"/>\n",
    "<br>\n",
    "(image from [Extrapolating](https://xkcd.com/605/))\n",
    "</center>\n",
    "\n",
    "So next task is to explore non-linear local models and non-parametric local models.\n",
    "\n",
    "\n",
    "### Parametric Models\n",
    "\n",
    "\n",
    "Recall the GTVMin in 3.20 \n",
    "\n",
    "$$\\left\\{\\widehat{\\mathbf{w}}^{(i)}\\right\\}_{i=1}^n \\in \\underset{\\text { stack }\\left\\{\\mathbf{w}^{(i)}\\right\\}_{i=1}^n}{\\operatorname{argmin}} \\sum_{i \\in \\mathcal{V}} L_i\\left(\\mathbf{w}^{(i)}\\right)+\\alpha \\sum_{\\left\\{i, i^{\\prime}\\right\\} \\in \\mathcal{E}} A_{i, i^{\\prime}}\\left\\|\\mathbf{w}^{(i)}-\\mathbf{w}^{\\left(i^i\\right)}\\right\\|_2^2$$\n",
    "\n",
    "We can decompose this objective function into $f^{(i)}(\\mathbf{w})$ corresponding to each node $i$, where $f^{(i)}(\\mathbf{w})$ contains local loss and neighbor regularization.\n",
    "\n",
    "$$\\widehat{\\mathbf{w}} \\in \\underset{\\mathbf{w} \\in \\mathbb{R}^{d \\cdot n}}{\\arg \\min } \\sum_{i \\in \\mathcal{V}} f^{(i)}(\\mathbf{w})$$\n",
    "\n",
    "with $f^{(i)}(\\mathbf{w}):=L_i\\left(\\mathbf{w}^{(i)}\\right)+(\\alpha / 2) \\sum_{i^{\\prime} \\in \\mathcal{N}^{(i)}} A_{i, i^{\\prime}}\\left\\|\\mathbf{w}^{(i)}-\\mathbf{w}^{\\left(i^{\\prime}\\right)}\\right\\|_2^2$\n",
    "\n",
    "\n",
    "Each $f^{(i)}$ involves only the parameters $\\mathbf{w}^{(i)}$ of node $i$ and its neighbors $\\mathcal{N}^{(i)}$. This means that while updating the model at node $i$, the contributions from distant nodes can be treated as fixed.\n",
    "\n",
    "**Update rule: (Algorithm 5.8)**\n",
    "\n",
    "<center>\n",
    "<img src=\"FedRelax_Para.png\" alt=\"FedRelax_Para\" width=\"80%\"/>\n",
    "</center>\n",
    "\n",
    "**5.24**:\n",
    "\n",
    "<center>\n",
    "<img src=\"Jacobi_update.png\" alt=\"Jaccobi_update\" width=\"80%\"/>\n",
    "</center>\n",
    "\n",
    "At each iteration $k$, every neighbor node $i^\\prime$ holds its current parameter estimate $\\widehat{\\mathrm{w}}_k^{(i^\\prime)}$. The update for node $i$ is computed by solving:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{w}}_{k+1}^{(i)} \\in \\underset{\\mathbf{w}^{(i)} \\in \\mathbb{R}^d}{\\operatorname{argmin}}\\left[L_i\\left(\\mathbf{w}^{(i)}\\right)+\\alpha \\sum_{i^{\\prime} \\in \\mathcal{N}^{(i)}} A_{i, i^{\\prime}}\\left\\|\\mathbf{w}^{(i)}-\\widehat{\\mathbf{w}}_k^{\\left(i^{\\prime}\\right)}\\right\\|_2^2\\right]\n",
    "$$\n",
    "\n",
    "The update is done in parallel for all nodes. This is akin to the Jacobi algorithm in numerical analysis, where every block coordinates is updated simultaneously based on the current values of the other coordinates. The optimization variables are partitioned into blocks (here, each node’s parameters) and each block is updated while treating the others as constants.\n",
    "\n",
    "\n",
    "\n",
    "### Non-parametric (or agnostic) model\n",
    "\n",
    "In non-parametric model, we don't have a vector of parameters to perform optimization methods. A general update rule for each node $i$ is given by\n",
    "\n",
    "$$\n",
    "\\widehat{h}_{k+1}^{(i)} \\in \\underset{h^{(0)} \\in \\mathcal{H}^{(0)}}{\\operatorname{argmin}}\\left\\{L_i\\left(h^{(i)}\\right)+\\alpha \\sum_{i^{\\prime} \\in \\mathcal{N}^{(i)}} A_{i, i^{\\prime}} D\\left(h^{(i)}, \\widehat{h}_k^{\\left(i^{\\prime}\\right)}\\right)\\right\\} .\n",
    "$$\n",
    "\n",
    "Using square error loss, we can represent $D$ as :\n",
    "\n",
    "$$D\\left(h^{(i)}, h^{\\left(i^{\\prime}\\right)}\\right):=\\left(1 / m^{\\prime}\\right) \\sum_{\\mathbf{x} \\in \\mathcal{D}^{\\left\\{i, i^{\\prime}\\right\\}}}\\left[h^{(i)}(\\mathbf{x})-h^{\\left(i^{\\prime}\\right)}(\\mathbf{x})\\right]^2$$\n",
    "\n",
    "In essence, each node updates its local model by minimizing a combination of its own loss and a penalty for deviating from the models of its neighbors. $\\widehat{h}_k^{\\left(i^{\\prime}\\right)}$ means in each $k+1$ iteration, we consider adjacent nodes as fixed model from $k$ iteration.\n",
    "\n",
    "**Update rule: (Algorithm 5.9)**\n",
    "\n",
    "<center>\n",
    "<img src=\"FedRelax_Agnostic.png\" alt=\"FedRelax_Agnostic\" width=\"80%\"/>\n",
    "</center>\n",
    "\n",
    "**5.25**:\n",
    "\n",
    "<center>\n",
    "<img src=\"Agnostic_update.png\" alt=\"Agnostic_update\" width=\"80%\"/>\n",
    "</center>\n",
    "\n",
    "**3.35**:\n",
    "\n",
    "<center>\n",
    "<img src=\"discrepancy.png\" alt=\"discrepancy\" width=\"80%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c11583b739548842897657a6195b533d",
     "grade": false,
     "grade_id": "cell-a1160f0fb0287182",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    " \n",
    "### Task 4.1 - Non-parametric Local Model -  Decision Tree\n",
    "\n",
    "__Task description:__\n",
    "\n",
    "\n",
    "Training process of Decision Tree:\n",
    "\n",
    "1. Start at the root node with all training data.\n",
    "\n",
    "2. Choose a feature that best splits the data into subgroups.\n",
    "\n",
    "3. Repeat step 2 recursively on each subgroup until some stopping criterion is met.\n",
    "\n",
    "Animation of Decision Tree:\n",
    "\n",
    "<center>\n",
    "<img src=\"DT_sine.gif\" alt=\"Animation of Decision Tree\" width=\"50%\"/>\n",
    "</center>\n",
    "\n",
    "A decision tree is a greedy algorithm that does not use backpropagation and typically lacks an \"intermediate state\" for iterative training (some variants support online training, we employ the vanilla version for universality)\n",
    "\n",
    "How can we train a decision tree in a federated style? \n",
    "\n",
    "Recall that the update rule:\n",
    "\n",
    "$$\\widehat{h}_{k+1}^{(i)} \\in \\underset{h^{(0)} \\in \\mathcal{H}^{(0)}}{\\operatorname{argmin}}\\left\\{L_i\\left(h^{(i)}\\right)+\\alpha \\sum_{i^{\\prime} \\in \\mathcal{N}^{(i)}} A_{i, i^{\\prime}} D\\left(h^{(i)}, \\widehat{h}_k^{\\left(i^{\\prime}\\right)}\\right)\\right\\}$$\n",
    "\n",
    "The first part is to achieve minimal local loss. \n",
    "\n",
    "The second part is to enhance the similarity between models of adjacent nodes. The similarity between models can be measured by comparing their outputs for identical inputs. To achieve this, we create an unlabeled \"public data\" set. Each node then utilizes \"augmented data,\" which combines its original data with public data, where the labels for the public data are derived from predictions made by neighboring nodes.\n",
    "\n",
    "<center>\n",
    "<img src=\"Data_aug.png\" alt=\"Data Augmentation\" width=\"50%\"/>\n",
    "</center>\n",
    "\n",
    "Define\n",
    "\n",
    "$$\n",
    "\\mathcal{D}^{(i^\\prime)}_\\text{public}:=\\left(X_\\text{public},h^{(i^\\prime)}(X_\\text{public})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "\\mathcal{D}^{(i,i^{\\prime})}_{aug} := \\mathcal{D}^{(i)} + \\sum_{i^{\\prime} \\in \\mathcal{N}^{(i)}}\\mathcal{D}^{(i^{\\prime})}_{public}\n",
    "$$\n",
    "\n",
    "Now we can update local model iteratively using Algorithm 5.9, where hypothesis $h$ is decision tree. \n",
    "\n",
    "\n",
    "__Hints:__\n",
    "\n",
    "You can follow (but not necessary) the process below.\n",
    "\n",
    "1. Train an initial tree solely on local data.\n",
    "2. In each iteration and each node:\n",
    "    - Build augmented data based on neighbors (you can average all neighbors' outputs as labels).\n",
    "    - Train a new tree using augmented data.\n",
    "\n",
    "!!! Always use `random_state = RANDOM_STATE` !!!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e848811142d78e27b41438d8be44a73",
     "grade": false,
     "grade_id": "cell-8f1d55a3afac3dec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fed_decision_tree(\n",
    "    graph,\n",
    "    max_iter=10,\n",
    "    max_depth=5\n",
    "):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    G = copy.deepcopy(graph)\n",
    "\n",
    "    # Create an unlabeled public dataset with uniform random values\n",
    "    # -33, 12 are the lowest and highest temperature in our dataset respectively \n",
    "    X_public = np.random.uniform(low=-33, high=12, size=(10, 10))\n",
    "\n",
    "    # Initialize each node's model on its local data (iteration k=0)\n",
    "    for node in G.nodes:\n",
    "\n",
    "        X_train = G.nodes[node]['X_train']\n",
    "        y_train = G.nodes[node]['y_train']\n",
    "        \n",
    "        # store the decision tree regressor model trained on the local dataset in the node's attribute 'model'. \n",
    "        # G.nodes[node]['model'] <- DecisionTreeRegressor fitted on local data\n",
    "        \n",
    "        # !!! in DecisionTreeRegressor, use random_state = RANDOM_STATE (which is 42) !!!\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Iteratively update for k in [0...max_iter-1]\n",
    "    for _ in range(max_iter):\n",
    "        # We'll store newly trained models in a temporary dict so that \n",
    "        # we don't overwrite the old models before neighbors can read them.\n",
    "        new_models = {}\n",
    "        \n",
    "        # For each node i, gather neighbors' predictions on X_public\n",
    "        for node in G.nodes:\n",
    "            neighbors = list(G.neighbors(node))\n",
    "            \n",
    "            # If no neighbors, we have no pseudo-labels\n",
    "            if len(neighbors) == 0:\n",
    "                # Just train on local data alone\n",
    "                X_aug = G.nodes[node]['X_train']\n",
    "                y_aug = G.nodes[node]['y_train']\n",
    "            else:\n",
    "                # Gather predictions from neighbors on X_public\n",
    "                # Average the predictions across neighbors\n",
    "\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "\n",
    "                # concat X_local and X_public, store them as augmented feature data\n",
    "                # X_aug <- concat(X_local, X_public)\n",
    "                # concat y_local and y_public, store them as augmented label data\n",
    "                # y_aug <- concat(y_local, y_public)\n",
    "                \n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            # Train a fresh tree on the augmented data\n",
    "            reg_new = DecisionTreeRegressor(max_depth=max_depth, random_state=RANDOM_STATE)\n",
    "            reg_new.fit(X_aug, y_aug)\n",
    "            \n",
    "            # Store in a local dict so we don't overwrite the old model\n",
    "            new_models[node] = reg_new\n",
    "        \n",
    "        # Synchronize: update each node's model with the newly trained model\n",
    "        for node in G.nodes:\n",
    "            G.nodes[node]['model'] = new_models[node]\n",
    "\n",
    "    # After max_iter iterations, each node's model is stored in G.nodes[node]['model'].\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can achieve a small MSE in most stations.\n",
    "# This is common in (naive) FL with heterogeneous data.\n",
    "# You can adjust max_iter and max_depth to see the trade-off between local loss and global loss.\n",
    "# With max_iter=10 and max_depth=5 and correct code, you can get avg_mse<50\n",
    "\n",
    "# !!! SO KEEP max_iter=10 AND max_depth=5 WHEN YOU SUBMIT !!! \n",
    "\n",
    "\n",
    "G_dt = fed_decision_tree(\n",
    "    G_FMI, \n",
    "    max_iter=10,\n",
    "    max_depth=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cccda8e7f5dbb1fd4677c6b3951afb1",
     "grade": false,
     "grade_id": "cell-ac61859ae825de50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MSE results\n",
    "\n",
    "mse_list = []\n",
    "for node in G_dt.nodes():\n",
    "    reg = G_dt.nodes[node]['model']\n",
    "    X_val_local = G_dt.nodes[node]['X_val']\n",
    "    y_val_local = G_dt.nodes[node]['y_val']\n",
    "    y_pred = reg.predict(X_val_local)\n",
    "    mse = mean_squared_error(y_val_local, y_pred)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(mse_list)), mse_list)\n",
    "plt.title('Per-Node Validation Error Distribution')\n",
    "plt.xlabel('Station Index')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()\n",
    "\n",
    "avg_mse = np.mean(mse_list)\n",
    "print(\"Average MSE:\", avg_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3be3fe5ffd47e4d0c013e7e510a36f5f",
     "grade": true,
     "grade_id": "cell-db9ed887befcbfad",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert(avg_mse<50), \"Try to achieve a better result.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a12afad8000d58f4e71c4f535acff3cf",
     "grade": false,
     "grade_id": "cell-059ebfab7e5a3b61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Question 4.1 - FedRelax\n",
    "\n",
    "__Formulation:__\n",
    "\n",
    "Why can't we use FedGD to train local decision tree models?\n",
    "    \n",
    "__Answer Options:__\n",
    "    \n",
    "1. Gradient descent is too costly in decision tree.\n",
    "2. Decision tree does not rely on gradient-based optimization.\n",
    "1. FedRelax is a superior algorithm to FedGD.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bcabd7ee92d6a467423b327ae1dcae7",
     "grade": false,
     "grade_id": "cell-6d6d084c52e563ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Question ###\n",
    "\n",
    "# Assign the variable to the answer option from the list above\n",
    "# answer = ?\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afdcc53961080ce88eb453a28ba02419",
     "grade": true,
     "grade_id": "cell-400ad73a66077592",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the chosen answer option is adequate\n",
    "assert answer in [1, 2, 3], \"Choose the answer option from the provided list.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84b42d61d81fe00b8aecd226b4062dfe",
     "grade": false,
     "grade_id": "cell-f76965d57d67a20c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Question 4.2 - Scaling factor\n",
    "\n",
    "__Formulation:__\n",
    "\n",
    "What are the effects of using a large value for scaling factor $\\alpha$?\n",
    "    \n",
    "__Answer Options:__\n",
    "    \n",
    "1. Using a large value for $\\alpha$ puts more emphasis on learning local model parameters that do not vary too much across edges of the empirical graph.\n",
    "\n",
    "2. The convergence is slower with larger $\\alpha$-value.  \n",
    "\n",
    "3. Using a large value for $\\alpha$ results in local model parameters with small values for the local loss (training error) at the expense of relatively large variations of model parameters across edges of the empirical graph.  \n",
    "\n",
    "4. Using a large value for $\\alpha$ puts more emphasis on learning local model parameters with small local loss.  \n",
    "\n",
    "5. Using a large value for $\\alpha$ results in local model parameters with small total variation at the expense of higher values for the local loss (training error).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a2bd654cf9b1a2cc33bcef70a2f85f5",
     "grade": false,
     "grade_id": "cell-12b36b12b7e85114",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Question ###\n",
    "\n",
    "# Assign the variable to the answer option from the list above\n",
    "# answer = {?, ?, ...}\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae3719917c730d2561bb57f9017c5b0c",
     "grade": true,
     "grade_id": "cell-d631f427b2cf84dd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the chosen answer option is adequate\n",
    "assert answer.issubset({1, 2, 3, 4, 5}), \"Choose the answer options from the provided list.\"\n",
    "assert isinstance(answer, set), \"Answer type should be set.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
