{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4c8e75-ca44-49d0-90c0-cf70af7be685",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e0f67b5bea53442f34ddfb7a4a3c00b",
     "grade": false,
     "grade_id": "cell-800179e4905382c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## CS-E4740 - Federated Learning D (Spring 25)\n",
    "\n",
    "# Assignment 1: ML Basics\n",
    "\n",
    "### R. Gafur, A. Jung\n",
    "\n",
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h2>Deadline: 17.03.2025</h2>\n",
    "</div>\n",
    "\n",
    "<a id='learning_goals'></a>\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Learning Goals\n",
    "<ul>\n",
    "    <li> Access weather data from the Finnish Meteorological Institute (FMI).</li>\n",
    "    <li> Utilize Python libraries (scikit-learn, Keras) to train basic machine learning (ML) models.</li>\n",
    "    <li> Implement regularization techniques via data augmentation.</li>\n",
    "</ul>\n",
    "\n",
    "## Backround Material\n",
    "\n",
    "- Chapter 2 of [FLBook (PDF)](https://github.com/alexjungaalto/FederatedLearning/blob/main/material/FLBook.pdf)\n",
    "- optional: [Linear model implementation in scikitlearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), [Decision tree implementation in scikitlearn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html), [Convolutional Neural Networks (CNN) implementation in Keras](https://www.tensorflow.org/tutorials/images/cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526a33d-6927-4022-a093-908604b97e59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd7760e0f9047594733dc3e7d8595903",
     "grade": false,
     "grade_id": "cell-286bdf1b024f8e3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "Before performing any data preprocessing or model training, we begin by loading the dataset and conducting an initial inspection.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load the dataset**: We read the weather data from a CSV file using `pandas`.\n",
    "2. **Check data types**: Displaying column data types helps us understand the structure of the dataset.\n",
    "3. **Inspect the first data point**: We print all feature values of the first record to get an overview of the dataset content.\n",
    "4. **Check dataset dimensions**: The dataset shape is printed to determine the number of rows and columns.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d667f5-cade-4e4c-8478-e308514057c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "366cdd45638974a9fcacec1872bd8b8e",
     "grade": false,
     "grade_id": "cell-01aa4e00280d75e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf  # for CNN model (tf.keras)\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.tree import DecisionTreeRegressor  # for Decision Tree Regressor\n",
    "from sklearn.linear_model import LinearRegression  # for Linear Regressoion\n",
    "from sklearn.metrics import mean_squared_error  # to measure mean squared error (MSE)\n",
    "from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv('weather_data.csv')\n",
    "\n",
    "# Check the data types and \n",
    "print(dataset.dtypes)\n",
    "print(\"\\n******************************\\n\")\n",
    "print(\"First data point:\")\n",
    "print(dataset.iloc[0])\n",
    "print(\"\\n******************************\\n\")\n",
    "print(f\"Dataset Shape: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b446c897-f6c0-4163-99a2-38a0163ae88b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "810f813eb3b545fbe3aba5303e525f52",
     "grade": false,
     "grade_id": "cell-1b12822da01a8aac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Turning raw data into datapoints, characterized by features and label. \n",
    "\n",
    "The dataset consists of hourly weather measurements, including:  \n",
    "- **Temperature**: Average, Maximum, Minimum \\([Â°C]\\)  \n",
    "- **Humidity**: Average relative humidity \\([\\%]\\)  \n",
    "- **Wind**: Speed, Maximum speed, Average direction \\([Â°]\\), Maximum gust speed \\([m/s]\\)  \n",
    "- **Precipitation**: Total precipitation \\([mm]\\)  \n",
    "- **Air Pressure**: Average air pressure \\([hPa]\\)  \n",
    "\n",
    "---\n",
    "### Data Points\n",
    "\n",
    "A data point corresponds to a specific hour, e.g., *06-April-2023 from 06:00 - 07:00*, which is recorded as `2023-04-06 06:00:00` (starting time) after preprocessing.\n",
    "\n",
    "- **Features** include all hourly observations from the **previous 5 hours**. Example: For the data point `2023-04-06 06:00:00`, the features correspond to data from *01:00 - 06:00* on the same day.\n",
    "\n",
    "- **Label** is the **temperature recorded 5 hours ahead**. Example: For the data point `2023-04-06 06:00:00`, the label corresponds to the temperature measured during *11:00 - 12:00*.\n",
    "\n",
    "- **Dataset Splits**:\n",
    "  - **Training Set**: Includes data from `2023-04-06 00:00:00` to `2023-04-08 00:00:00`.\n",
    "  - **Validation Set**: Comprises the remaining hours of 2023.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88142fc-2627-4ecd-ba78-13652279e365",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d536d459b8bad0730ebcd2dcb4e784a1",
     "grade": false,
     "grade_id": "cell-ff43b1c863eac8ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy the main dataset\n",
    "data = dataset.copy()\n",
    "\n",
    "# Convert specified columns to numeric (handling errors with 'coerce')\n",
    "numeric_columns = [\n",
    "    'Wind speed [m/s]', \n",
    "    'Maximum wind speed [m/s]', \n",
    "    'Average wind direction [Â°]', \n",
    "    'Maximum gust speed [m/s]'\n",
    "]\n",
    "data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill missing values with 0\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Create a 'timestamp' column by combining year, month, day, and local time\n",
    "data['timestamp'] = pd.to_datetime(\n",
    "    data['Year'].astype(str) + '-' +\n",
    "    data['Month'].astype(str) + '-' +\n",
    "    data['Day'].astype(str) + ' ' +\n",
    "    data['Time [Local time]']\n",
    ")\n",
    "\n",
    "# Identify columns for lagged features (excluding non-relevant ones)\n",
    "excluded_columns = ['Observation station', 'timestamp', 'Year', 'Month', 'Day', 'Time [Local time]']\n",
    "columns_to_lag = [col for col in data.columns if col not in excluded_columns]\n",
    "\n",
    "# Create lagged features for the previous 5 hours\n",
    "for lag in range(1, 6):\n",
    "    for col in columns_to_lag:\n",
    "        data[f\"{col} lag_{lag}\"] = data[col].shift(lag)\n",
    "\n",
    "# Define target variable (y) as the average temperature 5 hours ahead\n",
    "data['y'] = data['Average temperature [Â°C]'].shift(-5)\n",
    "\n",
    "# Drop rows with NaN values caused by shifts\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Print dataset shape\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9df296-7700-4f02-85e0-97b3cf6757ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01d9a249a139156320af2dbfe8471a30",
     "grade": false,
     "grade_id": "cell-84a763857e8d08c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check the data rows number\n",
    "assert data.shape[0] == 710, f\"Unexpected number of rows: {data.shape[0]}.\"\n",
    "\n",
    "print('Sanity check passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4253a9-8927-419b-9f82-49ca77977414",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00c2f6ec41f01d39fb13645dd0ec50e8",
     "grade": false,
     "grade_id": "cell-c39f04529d372b1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ðŸ“Œ TASK 1.1: Split the Data into Training and Validation Sets\n",
    "\n",
    "#### Task Description:\n",
    "1. **Split the dataset** into training (`X_train`, `y_train`) and validation (`X_val`, `y_val`) sets based on the specified time range.\n",
    "2. **Training Set**: Includes data from hours `2023-04-06 00:00:00` to (and including) `2023-04-08 00:00:00`.\n",
    "3. **Validation Set**: Consists of the remaining data.\n",
    "\n",
    "#### Hints:\n",
    "- Use **lagged feature columns** (i.e., columns containing `\"lag\"`) to create feature sets.\n",
    "- Assign **lagged columns** as the feature variables for `X_train` and `X_val`.\n",
    "- Use the **`y` column** as the target variable for `y_train` and `y_val`.\n",
    "    \n",
    "#### Points: 0.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25995df4-2518-4f2a-9a63-99724a8fc247",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd65db493764b31d9800af7086e7f605",
     "grade": false,
     "grade_id": "cell-a47cb8f7b000652c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_start = '2023-04-06 00:00:00'\n",
    "train_end = '2023-04-08 00:00:00' \n",
    "\n",
    "train_data = data[(data['timestamp'] >= train_start) & (data['timestamp'] <= train_end)]\n",
    "val_data = data[data['timestamp'] > train_end]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# X_train = \n",
    "# y_train = \n",
    "# X_val = \n",
    "# y_val = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}, X_val: {X_val.shape}, y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06281722-2079-44ee-9f0f-d519dcd5c5ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89f84d316777887969c0b78934744a35",
     "grade": true,
     "grade_id": "cell-502cffd8083f6ce4",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check the data rows number\n",
    "assert X_train.shape[0] == 49, f\"Unexpected number of rows for X_train: {X_train.shape[0]}.\"\n",
    "assert X_val.shape[0] == 546, f\"Unexpected number of rows for X_val: {X_val.shape[0]}.\"\n",
    "\n",
    "print('Sanity check passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da35b52-35ed-4323-bd54-30cd9f083dd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "001f85136c2210523fbe09aa029a7aac",
     "grade": false,
     "grade_id": "cell-22002a77953c697a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Preparing Data for Model Training: Standardization & Reproducibility\n",
    "\n",
    "After splitting the dataset into training and validation sets, the next step is to **prepare the features** for model training. Many machine learning models, especially those based on gradient-based optimization (e.g., linear regression, neural networks), perform better when features are standardized.\n",
    "\n",
    "#### Why Standardization?\n",
    "Feature standardization ensures that all input variables:\n",
    "- Have a **mean of 0** and **standard deviation of 1**, preventing features with different scales from disproportionately influencing the model.\n",
    "- Enable **faster and more stable convergence** during training.\n",
    "\n",
    "#### Steps:\n",
    "1. **Standardize the Features**:\n",
    "   - The `StandardScaler` from `scikit-learn` is used to transform both the training and validation datasets.\n",
    "   - `fit_transform()` is applied to `X_train` to compute and apply the transformation.\n",
    "   - `transform()` is applied to `X_val` to use the same scaling parameters as `X_train`, ensuring consistency.\n",
    "\n",
    "2. **Ensure Reproducibility**:\n",
    "   - To achieve consistent results across different runs, we set random seeds for both **NumPy** and **TensorFlow**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e2c13-358d-44b1-91ae-b17c39b80bef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b567c6bca43ca2b296e9f67a2f14c5d5",
     "grade": false,
     "grade_id": "cell-c4910f0826703290",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required library for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "# This ensures that the mean is 0 and the standard deviation is 1 for each feature in X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the validation set\n",
    "# We use transform() instead of fit_transform() to ensure the same scaling parameters from X_train are applied\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Set random seed for NumPy to ensure reproducibility in any random operations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set random seed for TensorFlow to ensure reproducibility in model training and initialization\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a77389-dd01-4dc0-9551-da5ee8673df5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0d51256d44d80a1d375ea40c163ad4e",
     "grade": false,
     "grade_id": "cell-6858472768b50e3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ðŸ“Œ TASK 1.2: Linear Regression\n",
    "\n",
    "#### Task Overview:\n",
    "Now that we have **standardized** our features to ensure proper scaling, we can proceed with training our first machine learning model. In this task, you will implement and evaluate a **linear regression model** using the standardized training and validation datasets.\n",
    "\n",
    "#### Task Instructions:\n",
    "1. **Train a Linear Regression Model** using `X_train_scaled` as input features.\n",
    "2. **Evaluate Model Performance** by computing the average squared error of the trained model on both the training (`X_train_scaled`,`y_train`) and validation (`X_val_scaled`,`y_val`) sets.\n",
    "\n",
    "#### Points: 1.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef66940-c1aa-4567-8473-04aaa026eb2b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "027a5f2a71cae43dbdd61c43ed656ec5",
     "grade": false,
     "grade_id": "cell-d0a40c1f3f1c707e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reg_train_error = \n",
    "# reg_val_error =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n*************** Linear Regression Diagnosis ***************\")\n",
    "print(f\"Training Error (MSE): {reg_train_error:.4f}\")\n",
    "print(f\"Validation Error (MSE): {reg_val_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92277c5d-9748-4b1c-ba6c-4d9b582f9b13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "037f862271e50afd0b80ca1ad9d49cd8",
     "grade": true,
     "grade_id": "cell-b12fa698e22d62e1",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity Checks: Ensuring Correctness of Model Evaluation\n",
    "\n",
    "# Check if computed errors are numeric values\n",
    "assert isinstance(reg_train_error, float), \"Error: reg_train_error must be a numeric value.\"\n",
    "assert isinstance(reg_val_error, float), \"Error: reg_val_error must be a numeric value.\"\n",
    "\n",
    "print(\"Sanity check passed! The computed errors are valid numerical values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c6d12-0074-4b87-babf-057f2b629e62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b5735265901e55e80199a99a7ce0655",
     "grade": false,
     "grade_id": "cell-2fc9504e694a74a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ðŸ“Œ TASK 1.3: Convolutional Neural Network (CNN)\n",
    "\n",
    "#### Task Overview:\n",
    "Now that we have trained a **linear regression model**, let's explore a more powerful approach: **a 1D Convolutional Neural Network (CNN)**. CNNs are well-suited for sequential data as they can capture local patterns and temporal dependencies.\n",
    "\n",
    "#### Task Instructions:\n",
    "1. **Implement a 1D CNN Model** using `tf.keras.Sequential` with `X_train_scaled` as input.\n",
    "2. **Define the CNN architecture** with the following layers:\n",
    "   - **Input Layer**: Shape `(X_train_scaled.shape[1], 1)`.\n",
    "   - **Multiple Conv1D Layers** with decreasing filter sizes and `ReLU` activation.\n",
    "   - **Flatten Layer** to convert feature maps into a vector.\n",
    "   - **Dense Output Layer** with a single neuron for regression.\n",
    "3. **Compile the Model** with:\n",
    "   - **Optimizer**: Adam\n",
    "   - **Loss Function**: Mean Squared Error (MSE)\n",
    "4. **Train the Model** on the training set.\n",
    "5. **Evaluate Model Performance** on both the training and validation sets (similar to Task 1.2).\n",
    "\n",
    "#### Hints:\n",
    "- Use `tf.keras.layers.Conv1D()` to define the convolutional layers with appropriate `filters`, `kernel_size`, and `activation`.  \n",
    "- Use `tf.keras.layers.Flatten()` to reshape the convolutional output before passing it to the dense layer.  \n",
    "- Use `tf.keras.layers.Dense()` for the final output layer.  \n",
    "- Construct the model using `tf.keras.Sequential()`, and compile it with `model.compile()`.  \n",
    "\n",
    "This CNN will serve as a more expressive alternative to linear regression, allowing us to compare their performances.\n",
    "\n",
    "#### Points: 2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9272bf-9d85-41dd-8e4f-4bc5344205ed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6d6edd6f0c812ac2e4002fa4374e2e",
     "grade": false,
     "grade_id": "cell-247c744a498f1bf1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cnn_model =\n",
    "# cnn_model.compile(...)\n",
    "# cnn_history =  \n",
    "\n",
    "# cnn_train_error =\n",
    "# cnn_val_error =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"\\n*************** Convolutional Neural Network (CNN) Diagnosis ***************\")\n",
    "print(\"Training error:\", cnn_train_error)\n",
    "print(\"Validation error:\", cnn_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28070c83-619e-43d9-90e7-2b8ed95d1494",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3304d2f8f488d583f6e9cd5191af2ab9",
     "grade": true,
     "grade_id": "cell-00123b0fc7edc4c6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(cnn_train_error, float), \"cnn_train_error must be a numeric value.\"\n",
    "assert isinstance(cnn_val_error, float), \"cnn_val_error must be a numeric value.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d015bbc-35fb-458d-8563-99df1095257d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "385fc9b5cedc0874b7e9979c1ddee50c",
     "grade": false,
     "grade_id": "cell-c69536fcc8dadf2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ðŸ“Œ TASK 1.4: Decision Tree Regressor\n",
    "\n",
    "#### Task Overview:\n",
    "Building on our previous models, we will now implement a **Decision Tree Regressor**, a non-parametric model that can capture nonlinear relationships in the data.\n",
    "\n",
    "#### Task Instructions:\n",
    "1. **Train a Decision Tree Regressor** using `X_train_scaled` as input features with `max_depth=3`.\n",
    "2. **Compute Mean Squared Error (MSE)** for training (`X_train_scaled`,`y_train`) and validation (`X_val_scaled`,`y_val`) sets.\n",
    "\n",
    "#### Points: 1.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7552de-4da0-480b-9843-5a3f004805b8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02b1a503eea2faabae63e267d5556acf",
     "grade": false,
     "grade_id": "cell-d9093756d3e2a7b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tree_train_error =\n",
    "# tree_val_error = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"\\n*************** Decision Tree Regressor Diagnosis ***************\")\n",
    "print(\"Training error:\", tree_train_error)\n",
    "print(\"Validation error:\", tree_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8297aa6-e501-4e00-b582-c21e57cf39c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3ff45fdb75d219e4399e9f43ca81ed1",
     "grade": true,
     "grade_id": "cell-ebda409a3c406b3f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(tree_train_error, float), \"tree_train_error must be a numeric value.\"\n",
    "assert isinstance(tree_val_error, float), \"tree_val_error must be a numeric value.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b104dcd-4e18-4775-9ae7-6e6605795b39",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4ff13728c8b455012602d09d8614741",
     "grade": false,
     "grade_id": "cell-a33c51b6d2c08951",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Regularization \n",
    "\n",
    "So far, we have trained **three basic ML models**, a linear model, a CNN and a decision tree. While these models can effectively learn patterns from the training data, they are **prone to overfitting**, especially when the dataset is small. To improve generalization, we can use **regularization techniques** such as adding the **ridge penalty**.\n",
    "\n",
    "### Implementing the Ridge Penalty with Data Augmentation  \n",
    "\n",
    "To implement **ridge regularization**, we will:\n",
    "1. **Generate an augmented training set** using independent and identically distributed (i.i.d.) **Gaussian random vectors**.\n",
    "2. **Set the labels** for this augmented data to **0**.\n",
    "3. **Modify our existing models** (Linear Regression, CNN, and Decision Tree) to incorporate this augmented dataset.\n",
    "\n",
    "This approach introduces **a penalty term**, encouraging models to keep their predictions closer to zero for these artificial data points, thus improving stability and reducing overfitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6a741-6d78-4142-91e6-a0c034773f22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75c14f4ffd334893753633895fd64bc5",
     "grade": false,
     "grade_id": "cell-1e50876935e180af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_augmented_dataset(X_train, y_train, augmentation_ratio=0.5, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generates an augmented training set by adding synthetic data points drawn from a Gaussian distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (numpy array or DataFrame): Original training feature set.\n",
    "    - y_train (numpy array or Series): Original training labels.\n",
    "    - augmentation_ratio (float): Ratio of synthetic samples to real training samples (default: 0.5).\n",
    "    - random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_aug (numpy array): Augmented feature set.\n",
    "    - y_train_aug (numpy array): Augmented labels (including original and synthetic samples).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seed for full reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)  # Ensure reproducibility in TensorFlow if used later\n",
    "\n",
    "    # Determine the number of augmented samples\n",
    "    num_augmented_samples = int(augmentation_ratio * X_train.shape[0])\n",
    "\n",
    "    # Compute mean and standard deviation of each feature in X_train\n",
    "    feature_means = np.mean(X_train, axis=0)\n",
    "    feature_stds = np.std(X_train, axis=0) + 1e-8  # Small epsilon to avoid zero variance issues\n",
    "\n",
    "    # Generate synthetic feature vectors from a Gaussian distribution\n",
    "    X_augmented = np.random.normal(loc=feature_means, scale=feature_stds, size=(num_augmented_samples, X_train.shape[1]))\n",
    "\n",
    "    # Set the labels for the augmented data points to 0\n",
    "    y_augmented = np.zeros(num_augmented_samples)\n",
    "\n",
    "    # Combine the original and augmented datasets\n",
    "    X_train_aug = np.vstack((X_train, X_augmented))\n",
    "    y_train_aug = np.hstack((y_train, y_augmented))\n",
    "\n",
    "    return X_train_aug, y_train_aug\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train_scaled and y_train are already defined\n",
    "X_train_aug, y_train_aug = generate_augmented_dataset(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Original training set shape:\", X_train_scaled.shape)\n",
    "print(\"Augmented training set shape:\", X_train_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc850685-fc20-46ba-9130-d0c2e881c634",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ecbc646bdc0f27dea5738da50effc5e",
     "grade": false,
     "grade_id": "cell-86a58c1344835504",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='regularized_models'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ðŸ“Œ TASK 1.5: Regularized Model Training with Augmented Data\n",
    "\n",
    "In this task, you will **regularize the training** of all three models from previous tasksâ€”**Linear Regression, CNN, and Decision Tree Regressor**â€”by replacing the original training set with the **augmented training set** generated above. The augmented dataset contains **synthetic feature vectors drawn from a Gaussian distribution**, with their labels set to **0**. For parametric models, using this augmented dataset to train ML a model is equivalent to adding a ridge penalty term (see Sec. 6.6. of [MLBook (PDF)](https://mlbook.cs.aalto.fi)).\n",
    "\n",
    "#### Task Instructions:\n",
    "1. **Train the following models using the augmented training set (`X_train_aug`, `y_train_aug`)**:\n",
    "   - **Linear Regression**\n",
    "   - **Convolutional Neural Network (CNN)**\n",
    "   - **Decision Tree Regressor**\n",
    "2. **Compute Mean Squared Error (MSE)** of these regularized models on the original training (`X_train_scaled`,`y_train`) and validation sets (`X_val_scaled`,`y_val`).\n",
    "\n",
    "#### Points: 1.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa57f9a-88b8-49eb-bc2f-12b7b13d041b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2792690b475d42157fea35acc6de50f",
     "grade": false,
     "grade_id": "cell-e04b32ef36edd4ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Linear Regression using augmented training set\n",
    "# custom_reg =\n",
    "\n",
    "# Convolutional Neural Network (CNN) using augmented training set\n",
    "# custom_cnn =\n",
    "# custom_cnn.compile(...)\n",
    "# custom_cnn =  \n",
    "\n",
    "# Decision Tree Regressor using augmented training set\n",
    "# custom_tree =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "custom_reg_train_error = mean_squared_error(y_train, custom_reg.predict(X_train_scaled)) \n",
    "custom_reg_val_error = mean_squared_error(y_val, custom_reg.predict(X_val_scaled)) \n",
    "\n",
    "print(\"\\n*************** Diagnosis of Regularized Linear Model ***************\")\n",
    "print(\"Training error:\", custom_reg_train_error)\n",
    "print(\"Validation error:\", custom_reg_val_error)\n",
    "\n",
    "custom_cnn_train_error = custom_cnn.evaluate(X_train_scaled[..., np.newaxis], y_train, verbose=0)\n",
    "custom_cnn_val_error = custom_cnn.evaluate(X_val_scaled[..., np.newaxis], y_val, verbose=0)\n",
    "\n",
    "print(\"\\n*************** Diagnosis of Regularized CNN ***************\")\n",
    "print(\"Training error:\", custom_cnn_train_error)\n",
    "print(\"Validation error:\", custom_cnn_val_error)\n",
    "\n",
    "custom_tree_train_error = mean_squared_error(y_train, custom_tree.predict(X_train_scaled))\n",
    "custom_tree_val_error = mean_squared_error(y_val, custom_tree.predict(X_val_scaled))\n",
    "\n",
    "print(\"\\n*************** Diagnosis of Regularized Decision Tree ***************\")\n",
    "print(\"Training error:\", custom_tree_train_error)\n",
    "print(\"Validation error:\", custom_tree_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785a988-b4cc-40ff-9b2b-66bf46c0a696",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72e87c65743bfa629d2a3f5241c227b0",
     "grade": true,
     "grade_id": "cell-7679a24953932c7f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(reg_train_error, float), \"custom_reg_train_error must be a numeric value.\"\n",
    "assert isinstance(reg_val_error, float), \"custom_reg_val_error must be a numeric value.\"\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(cnn_train_error, float), \"cnn_train_error must be a numeric value.\"\n",
    "assert isinstance(cnn_val_error, float), \"cnn_val_error must be a numeric value.\"\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(tree_train_error, float), \"tree_train_error must be a numeric value.\"\n",
    "assert isinstance(tree_val_error, float), \"tree_val_error must be a numeric value.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55c6d4-511e-4bf0-a87b-9fe4843d9170",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b7a3b95f0cebf98b4ce32bad6160af0",
     "grade": false,
     "grade_id": "cell-9ac5ab50b779305c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51371567-0b10-436b-8908-408c27a60fae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b812fd35564dd0c99766833d34085bda",
     "grade": false,
     "grade_id": "cell-5d9a1624d9fde6b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define model names\n",
    "models = [\"Linear Model\", \"CNN\", \"Decision Tree\"]\n",
    "\n",
    "# Training errors\n",
    "train_errors = [reg_train_error, cnn_train_error, tree_train_error]\n",
    "custom_train_errors = [custom_reg_train_error, custom_cnn_train_error, custom_tree_train_error]\n",
    "\n",
    "# Validation errors\n",
    "val_errors = [reg_val_error, cnn_val_error, tree_val_error]\n",
    "custom_val_errors = [custom_reg_val_error, custom_cnn_val_error, custom_tree_val_error]\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "error_data = pd.DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Training Error (Original)\": train_errors,\n",
    "    \"Training Error (Regularized)\": custom_train_errors,\n",
    "    \"Validation Error (Original)\": val_errors,\n",
    "    \"Validation Error (Regularized)\": custom_val_errors\n",
    "})\n",
    "\n",
    "# Print the table\n",
    "print(error_data.to_string(index=False))\n",
    "\n",
    "# Define x-axis positions for grouped bar charts\n",
    "x = np.arange(len(models))  # Model positions\n",
    "width = 0.3  # Bar width for better visualization\n",
    "\n",
    "# Plot Training Errors\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x - width/2, train_errors, width, label='Original Model', color='blue', alpha=0.7)\n",
    "plt.bar(x + width/2, custom_train_errors, width, label='Regularized Model', color='red', alpha=0.7)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Training Error\")\n",
    "plt.title(\"Training Errors (Regularized vs. Non-Regularized)\")\n",
    "plt.xticks(ticks=x, labels=models)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Validation Errors\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x - width/2, val_errors, width, label='Original Model', color='blue', alpha=0.7)\n",
    "plt.bar(x + width/2, custom_val_errors, width, label='Regularized Model', color='red', alpha=0.7)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.title(\"Validation Errors (Regularized vs. Non-Regularized)\")\n",
    "plt.xticks(ticks=x, labels=models)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
