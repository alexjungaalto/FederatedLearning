{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462453b9-de88-466a-aadf-af4a4cc891dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c2cedfba2d987409ab3c2c53aff0a64",
     "grade": false,
     "grade_id": "cell-6ab1b9e9b5c383d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## CS-E4740 - Federated Learning D (Spring 25)\n",
    "\n",
    "# Assignment 6: Trustworthy FL\n",
    "\n",
    "### A. Pavlyuk, A. Jung, and ChatGPT\n",
    "\n",
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h2>Deadline: 14.04.2025</h2>\n",
    "</div>\n",
    "\n",
    "<a id='varying_features'></a><div class=\"alert alert-info\">\n",
    "\n",
    "## Learning Goals\n",
    "After completing the notebook, you should\n",
    "    \n",
    "- know how to ensure subjective explainability in GTV minimization (GTVMin) based methods,\n",
    "- be aware of privacy attacks on GTVMin-based methods.\n",
    "\n",
    "\n",
    "## Background Material\n",
    "\n",
    "1. Chapters 8 and 9 of [FLBook](https://github.com/alexjungaalto/FederatedLearning/blob/main/material/FLBook.pdf)\n",
    "2. [Documentation of Python library networkx](https://networkx.org/)\n",
    "\n",
    "## Notebook Structure\n",
    "1. The notebook consists of two parts: coding tasks (referred to as Task) and quiz questions (referred as Question). \n",
    "2. Both Tasks and Questions use a point-separated index X.Y, where X is the number of the notebook and Y is the number of the task/question in the X'th notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f23f50-f4a2-456f-9666-67119fd840cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b4849b70d2816481d1a89ff538ece72",
     "grade": false,
     "grade_id": "cell-5f8dcc0652c83709",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263c7cd-2a3b-49a0-b534-34168954e122",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c90522ddbb46449191050e49d33ca4a6",
     "grade": false,
     "grade_id": "cell-d290f000c22e7331",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import copy # Provides shallow and deep copy operations for objects\n",
    "import numpy as np # NumPy for numerical computations and array manipulations\n",
    "import pandas as pd # Pandas for data manipulation and analysis\n",
    "from datetime import datetime # Handles date and time operations\n",
    "from numpy import linalg as LA # Linear algebra functions\n",
    "import matplotlib.pyplot as plt # Matplotlib for data visualization\n",
    "\n",
    "# Scikit-learn methods\n",
    "from sklearn.neighbors import kneighbors_graph # Computes the k-nearest neighbor graph\n",
    "from sklearn.metrics import mean_squared_error # Measures the mean squared error for regression tasks\n",
    "from sklearn.preprocessing import StandardScaler # Standardizes features by removing the mean and scaling to unit variance\n",
    "from sklearn.model_selection import train_test_split # Splits datasets into training and testing subsets\n",
    "\n",
    "# We will use NetworkX objects to store empirical graphs, local datasets, and models.\n",
    "import networkx as nx # NetworkX helps in creating, manipulating, and analyzing graph structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3fd87d-b620-4a83-90cd-4aec992eacc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bd0f95f96d96307fdff9c54dc567295",
     "grade": false,
     "grade_id": "cell-65ff6b15b59c22ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48746b3a-0a88-4591-965f-679803f79cb0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d9356c1f9eb27590fb318ab93702e78",
     "grade": false,
     "grade_id": "cell-66aeee273cef1bfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotFMI(G, show_IDs=True, nodes_red=[]): \n",
    "    \"\"\"\n",
    "    Plots FMI stations on a scatter plot, with options to highlight specific nodes in red \n",
    "    and label them using either user names or node IDs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        A graph where each node represents an FMI station, containing:\n",
    "        - 'coord' (tuple: latitude, longitude) for spatial positioning.\n",
    "        - 'user_name' (str or None) for optional labeling.\n",
    "\n",
    "    show_IDs : bool, optional (default=True)\n",
    "        If True, labels nodes without a 'user_name' attribute using their index.\n",
    "\n",
    "    nodes_red : list of int, optional (default=[])\n",
    "        A list of node indices to be highlighted in red.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays a scatter plot where:\n",
    "        - Nodes with a 'user_name' are colored blue and labeled with their name.\n",
    "        - Nodes without a 'user_name' are black (unless in `nodes_red`, then red).\n",
    "        - Edges are drawn in gray.\n",
    "        - The plot is restricted to the coordinate range (18.5 ≤ longitude ≤ 31.5, 59 ≤ latitude ≤ 71).\n",
    "    \"\"\"\n",
    "    # Extract coordinates\n",
    "    coords = np.array([G.nodes[node]['coord'] for node in G.nodes])\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Add nodes with labels\n",
    "    for node, (lat, lon) in enumerate(coords):\n",
    "        user_name = G.nodes[node]['user_name']\n",
    "        if user_name != None:\n",
    "            if node in nodes_red:\n",
    "                ax.scatter(coords[node, 1], coords[node, 0], color='red', s=10, zorder=5)\n",
    "            else:\n",
    "                ax.scatter(coords[node, 1], coords[node, 0], color='blue', s=10, zorder=5)\n",
    "            ax.text(lon + 0.7, lat - 0.5, user_name, fontsize=16, ha='center', va='center', color='blue', fontweight='bold')\n",
    "        else:\n",
    "            if node in nodes_red:\n",
    "                ax.scatter(coords[node, 1], coords[node, 0], color='red', s=10, zorder=5)\n",
    "            else:\n",
    "                ax.scatter(coords[node, 1], coords[node, 0], color='black', s=10, zorder=5)\n",
    "            if show_IDs:\n",
    "                ax.text(lon + 0.1, lat + 0.2, str(node), fontsize=8, ha='center', va='center', color='black', fontweight='bold')\n",
    "\n",
    "    # Draw edges\n",
    "    for u, v in G.edges:\n",
    "        ax.plot([coords[u, 1], coords[v, 1]], [coords[u, 0], coords[v, 0]], linestyle='-', color='gray')\n",
    "        \n",
    "    # Set axis limits\n",
    "    ax.set_xlim(18.5, 31.5)\n",
    "    ax.set_ylim(59, 71)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('longitude')\n",
    "    ax.set_ylabel('latitude')\n",
    "    ax.set_title('FMI stations')\n",
    "    \n",
    "    # Stretch according to Tissot's indicatrix\n",
    "    ax.set_aspect(1.6)\n",
    "    plt.show()\n",
    "    \n",
    "def add_edges(G, numneighbors=4):\n",
    "    \"\"\"Adds edges to a graph based on k-nearest neighbors using station coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        A graph where each node has a 'coord' attribute with (latitude, longitude).\n",
    "    numneighbors : int, optional\n",
    "        Number of nearest neighbors to connect to each node, by default 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    networkx.Graph\n",
    "        A new graph with added edges based on k-nearest neighbors.\n",
    "    \"\"\"\n",
    "    # Deep copy the graph to avoid modifying the original\n",
    "    graph_with_edges = copy.deepcopy(G)\n",
    "\n",
    "    # Extract coordinates\n",
    "    coords = np.array([graph_with_edges.nodes[node]['coord'] for node in graph_with_edges.nodes])\n",
    "\n",
    "    # Create adjacency matrix using k-nearest neighbors\n",
    "    adjacency_matrix = kneighbors_graph(coords, numneighbors, mode='connectivity', include_self=False)\n",
    "\n",
    "    # Add edges based on the adjacency matrix\n",
    "    edges = zip(*adjacency_matrix.nonzero())\n",
    "    graph_with_edges.add_edges_from(edges)\n",
    "\n",
    "    return graph_with_edges\n",
    "\n",
    "def ExtractFeatureMatrixLabelVector(data):\n",
    "    \"\"\"Extracts feature matrix and label vector from FMI weather data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame containing columns \"Latitude\", \"Longitude\", \"temp\", \"Timestamp\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix where each row corresponds to a data point.\n",
    "    y : numpy.ndarray\n",
    "        Label vector where each entry corresponds to the temperature value of a data point.\n",
    "    \"\"\"\n",
    "    # Extract temperature and normalize the latitude and longitude\n",
    "    temps = data['temp'].values\n",
    "    latitudes = data['Latitude'].values / 100\n",
    "    longitudes = data['Longitude'].values / 100\n",
    "\n",
    "    # Extract time-based features (year, month, day, hour, minute)\n",
    "    timestamps = data['Timestamp']\n",
    "    year = timestamps.dt.year / 2025\n",
    "    month = timestamps.dt.month / 13\n",
    "    day = timestamps.dt.day / 32\n",
    "    hour = timestamps.dt.hour / 25\n",
    "    minute = timestamps.dt.minute / 61\n",
    "\n",
    "    # Combine features into the feature matrix\n",
    "    X = np.column_stack([latitudes, longitudes, year, month, day, hour, minute])\n",
    "    y = temps.reshape(-1, 1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67aa04-6a6b-4e5f-8e46-c24f0f126e20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a9b765375dd1b23fcf0900ceb11ef17",
     "grade": false,
     "grade_id": "cell-82c3be8e14fb3481",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf55566-4d7a-4df5-b70b-841be35397cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "693ae6186f28ffe0e419e92e013d7cb2",
     "grade": false,
     "grade_id": "cell-81f332615b6ece33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the random seed to be used everywhere\n",
    "seed = 4740"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc2a1d-1a87-48c6-983d-1406b573b0e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a48265765eb6eeca13f5ceac00cee47c",
     "grade": false,
     "grade_id": "cell-049b3a9aaf8e172d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf374bf-aa29-42de-bcc8-d6af189dca6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6103456e3fd21b9b856488f5e5ec3bb",
     "grade": false,
     "grade_id": "cell-a0fb72432597632c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the weather measurements\n",
    "data = pd.read_csv('FMI_data.csv')\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "\n",
    "# Define the number of the unique stations\n",
    "n_stations = len(data.name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e587e0-a4e9-4ca1-9f2f-98737adee944",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d0248f5436bcac544bdea7ea6f95ab0",
     "grade": false,
     "grade_id": "cell-35ac644b4017b708",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Building an FMI Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3255cd-1cb2-49c6-bddc-efae244b6f85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aea9cb93045a92431292cff27848d7bb",
     "grade": false,
     "grade_id": "cell-cb7db24cd84df724",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a networkX graph\n",
    "G_FMI_no_edges = nx.Graph()\n",
    "\n",
    "# Add a one node per station\n",
    "G_FMI_no_edges.add_nodes_from(range(n_stations))\n",
    "\n",
    "for i, station in enumerate(data.name.unique()):\n",
    "    # Extract data of a certain station\n",
    "    station_data = data[data.name==station]\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X_node, y_node = ExtractFeatureMatrixLabelVector(station_data)\n",
    "    \n",
    "    # Split the dataset into training and validation set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_node, y_node, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Assign node attributes\n",
    "    G_FMI_no_edges.nodes[i].update({\n",
    "        'name': station, # The name of the i-th weather station\n",
    "        'user_name': None, # The name of user at i-th weather station\n",
    "        'coord': (station_data.Latitude.iloc[0], station_data.Longitude.iloc[0]), # The coordinates of the i-th weather station\n",
    "        'X_train': X_train, # The training feature matrix for local dataset at node i\n",
    "        'y_train': y_train, # The training label vector for local dataset at node i\n",
    "        'X_test': X_test, # The test feature matrix for local dataset at node i\n",
    "        'y_test': y_test, # The test label vector for local dataset at node i\n",
    "    })\n",
    "    \n",
    "# Add edges between each station and its nearest neighbors.\n",
    "# NOTE: the node degree might be different for different nodes\n",
    "G_FMI = add_edges(G_FMI_no_edges, numneighbors=4)\n",
    "print(\"The empirical graph is connected:\", nx.is_connected(G_FMI))\n",
    "\n",
    "# Visualize the empirical graph\n",
    "plotFMI(G_FMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475ad9c-5db4-4f2a-916e-7db49c1150e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a00aeac15b423e1c831ff5f729c52cb",
     "grade": false,
     "grade_id": "cell-d828b9c8eb89b336",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## FedGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aedca6-dc4a-413f-8eff-ab73ccdb8fcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab078ff0d3ecf9554986bb36b145d415",
     "grade": false,
     "grade_id": "cell-3dfc64ac632be09f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FedGD(graph_FMI, \n",
    "          alpha=0.5, \n",
    "          l_rate=0.1, \n",
    "          max_iter=1000, \n",
    "          tol=0.0001, \n",
    "          logging=False):\n",
    "    \"\"\"\n",
    "    Performs Federated Gradient Descent (FedGD) on a network of FMI stations, \n",
    "    where each node updates its local model while incorporating neighbor influence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_FMI : networkx.Graph\n",
    "        A graph where each node represents a station with:\n",
    "        - 'X_train' (numpy.ndarray): Feature matrix for local training.\n",
    "        - 'y_train' (numpy.ndarray): Target values.\n",
    "        - 'weights' (numpy.ndarray): Initial weight vector (initialized as zeros).\n",
    "    \n",
    "    alpha : float, optional (default=0.5)\n",
    "        Regularization parameter controlling the influence of neighboring nodes.\n",
    "\n",
    "    l_rate : float, optional (default=0.1)\n",
    "        Learning rate for the gradient descent updates.\n",
    "\n",
    "    max_iter : int, optional (default=1000)\n",
    "        Maximum number of iterations for gradient descent.\n",
    "\n",
    "    tol : float, optional (default=0.0001)\n",
    "        Convergence threshold based on the average weight change.\n",
    "\n",
    "    logging : bool, optional (default=False)\n",
    "        If True, prints the average weight change at each iteration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    graph : networkx.Graph\n",
    "        A modified copy of `graph_FMI` where each node's 'weights' attribute \n",
    "        contains the trained model parameters.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The algorithm performs synchronous updates across nodes.\n",
    "    - The stopping criterion is based on weight change convergence.\n",
    "    - If `max_iter` is reached, a message is printed indicating termination.\n",
    "    \"\"\" \n",
    "    # Deep copy the input graph\n",
    "    graph = copy.deepcopy(graph_FMI)\n",
    "\n",
    "    # Initialize weights at each node with zeros\n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node]['weights'] = np.zeros((graph.nodes[node]['X_train'].shape[1], 1))\n",
    "\n",
    "    # Federated gradient descent\n",
    "    prev_avg_delta_w = 1001\n",
    "    curr_avg_delta_w = 1000\n",
    "    i = 0\n",
    "    while i < max_iter and prev_avg_delta_w - curr_avg_delta_w > tol:\n",
    "        # Compute updated weights for all nodes\n",
    "        delta_w = {}\n",
    "        updates = {}\n",
    "        for node in graph.nodes:\n",
    "            X_train, y_train = graph.nodes[node]['X_train'], graph.nodes[node]['y_train']\n",
    "            w_current = graph.nodes[node]['weights']\n",
    "            term_1 = (2 / len(y_train)) * X_train.T.dot(X_train.dot(w_current) - y_train)\n",
    "            term_2 = 2 * alpha * sum(\n",
    "                w_current - graph.nodes[neighbor]['weights'] for neighbor in graph.neighbors(node)\n",
    "            )\n",
    "            updates[node] = w_current - l_rate * (term_1 + term_2)\n",
    "\n",
    "        # Apply updates synchronously\n",
    "        for node, new_weights in updates.items():\n",
    "            delta_w[node] = np.linalg.norm(graph.nodes[node]['weights'] - new_weights)\n",
    "            graph.nodes[node]['weights'] = new_weights\n",
    "\n",
    "        # Calculate the average of weight differences\n",
    "        prev_avg_delta_w = curr_avg_delta_w\n",
    "        curr_avg_delta_w = np.array(list(delta_w.values())).mean()\n",
    "        if logging:\n",
    "            print(f\"Iteration #{i}: avg_delta_w = {curr_avg_delta_w}\")\n",
    "\n",
    "        # Iteration step\n",
    "        i += 1\n",
    "\n",
    "    # Logging\n",
    "    if i == max_iter:\n",
    "        print(f\"Maximum iteration reached\\n\")\n",
    "    else:\n",
    "        print(f\"Convergence reached at iteration #{i}\\n\")\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5bc78-f231-451a-ac3c-eac5d8b7c190",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9e761755d494e53e14c2a6ce757a770",
     "grade": false,
     "grade_id": "cell-adc00a27d4efa93c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## FedGD with Subjective Explainability\n",
    "\n",
    "One of the key requirements for trustworthy AI is __Transparency__ (see Section 8.2.4 in [FLBook](https://github.com/alexjungaalto/FederatedLearning/blob/main/material/FLBook.pdf)). The transparent FL system must include good explainability of the trained local models. We can measure the explainability of the trained model by comparing the prediciton with the guess made by the user. Therefore, the penalty term can be added to the local loss function:\n",
    "\n",
    "$$\n",
    "L_i(\\mathbf{w}^{(i)}) := \\underbrace{\\frac{1}{m_i} \\| \\mathbf{X}^{(i)}\\mathbf{w}^{({i})} - \\mathbf{y}^{(i)}\\|_2^2}_{\\text{training error}} + \\underbrace{\\rho \\frac{1}{|D_t^{(i)}|} \\sum_{\\mathbf{x} \\in D_t^{(i)}}{\\left(\\mathbf{x}^T \\mathbf{w}^{(i)} - u^{(i)}(\\mathbf{x})\\right)^2}}_{\\text{subjective explainability}}\n",
    "$$\n",
    "\n",
    "Now, we can formulate the gradient step with Subjective Explainability penalty term:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(k+1)} := \\mathbf{w}^{(k)} - \\eta \\nabla f(\\mathbf{w}^{(k)}) \\text{, where}\\\\\n",
    "\\{ \\hat{\\mathbf{w}}^{(i)}\\}_{i=1}^n \\in \\underset{\\{\\mathbf{w}^{(i)}\\}}{\\text{argmin}} \\underbrace{\\sum_{i \\in \\mathcal{V}}{\\overbrace{L_i(\\mathbf{w}^{(i)})}^{\\text{local loss}} + \\alpha \\sum_{\\{i, i'\\} \\in \\mathcal{E}}{A_{i, i'} \\|\\mathbf{w}^{(i)}-\\mathbf{w}^{(i')}\\|_2^2}}}_{=: f(\\mathbf{w})}\n",
    "\\\\\n",
    "\\text{and}\n",
    "\\\\\n",
    "L_i(\\mathbf{w}^{(i)}) := \\underbrace{\\frac{1}{m_i} \\| \\mathbf{X}^{(i)}\\mathbf{w}^{({i})} - \\mathbf{y}^{(i)}\\|_2^2}_{\\text{training error}} + \\underbrace{\\rho \\frac{1}{|D_t^{(i)}|} \\sum_{\\mathbf{x} \\in D_t^{(i)}}{\\left(\\mathbf{x}^T \\mathbf{w}^{(i)} - u^{(i)}(\\mathbf{x})\\right)^2}}_{\\text{subjective explainability}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e595857-064a-48af-99ef-d31ec16cd5da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "752fc077bc46a57b12d06206edd86083",
     "grade": false,
     "grade_id": "cell-4ce5ce31fe06fe25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Task 6.1 - Subjective Explainability\n",
    "    \n",
    "__Task description:__ \n",
    "1. Calculate the gradient of the Subjective Explainability penalty term.\n",
    "2. Modify the previously defined `FedGD` function according to the gradient step with Subjective Explainability formula.\n",
    "    \n",
    "__Hints:__ \n",
    "1. Read Section 8.4 in the [FLBook](https://github.com/alexjungaalto/FederatedLearning/blob/main/material/FLBook.pdf).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d4862-2f9e-465b-a799-85ebb1f8cb00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0829944b6fc63cd8ead9cbcd09476dd9",
     "grade": false,
     "grade_id": "cell-f613f4e85fc39f96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's assume the subjective explainability is sufficient.\n",
    "# Therefore, we define it by adding the noise to the true values.\n",
    "np.random.seed(seed)\n",
    "for node in G_FMI.nodes:\n",
    "    y_test = G_FMI.nodes[node]['y_test']\n",
    "    noise = y_test * np.random.uniform(-0.01, 0.01)  # Noise is ±10% of true values\n",
    "    G_FMI.nodes[node]['u'] = y_test + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7334be5-20ce-4533-86dd-2fd64dce999b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4788bcdec56c8cba7b7815d1be2e0022",
     "grade": false,
     "grade_id": "cell-7f4783b2a7baa00d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FedGD_Exp(graph_FMI, \n",
    "              alpha=0.5, \n",
    "              rho=0.5, \n",
    "              l_rate=0.05, \n",
    "              max_iter=1000, \n",
    "              tol=0.0001, \n",
    "              logging=False):\n",
    "    \"\"\"\n",
    "    Performs an extended version of Federated Gradient Descent (FedGD) incorporating \n",
    "    subjective explainability constraints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_FMI : networkx.Graph\n",
    "        A graph where each node represents an FMI station, containing:\n",
    "        - 'X_train' (numpy.ndarray): Feature matrix for local training.\n",
    "        - 'y_train' (numpy.ndarray): Target values for training.\n",
    "        - 'X_test' (numpy.ndarray): Feature matrix for subjective explainability.\n",
    "        - 'u' (numpy.ndarray): User signal values indicating explainability preferences.\n",
    "        - 'weights' (numpy.ndarray): Initialized weight vector (set to zeros).\n",
    "\n",
    "    alpha : float, optional (default=0.5)\n",
    "        Regularization parameter controlling the influence of neighboring nodes.\n",
    "\n",
    "    rho : float, optional (default=0.5)\n",
    "        Weight for the subjective explainability term in the gradient update.\n",
    "\n",
    "    l_rate : float, optional (default=0.05)\n",
    "        Learning rate for the gradient descent updates.\n",
    "\n",
    "    max_iter : int, optional (default=1000)\n",
    "        Maximum number of iterations for gradient descent.\n",
    "\n",
    "    tol : float, optional (default=0.0001)\n",
    "        Convergence threshold based on the average weight change.\n",
    "\n",
    "    logging : bool, optional (default=False)\n",
    "        If True, prints the average weight change at each iteration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    graph : networkx.Graph\n",
    "        A modified copy of `graph_FMI` where each node's 'weights' attribute \n",
    "        contains the trained model parameters.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The algorithm performs synchronous updates across all nodes.\n",
    "    - The weight update rule consists of:\n",
    "        (I) Training error gradient (local loss minimization).\n",
    "        (II) Subjective explainability gradient (user preference enforcement).\n",
    "        (III) Graph regularization gradient (neighboring influence).\n",
    "    - The stopping criterion is based on weight change convergence.\n",
    "    - If `max_iter` is reached, a message is printed indicating termination.\n",
    "    \"\"\"\n",
    "    # Deep copy the input graph\n",
    "    graph = copy.deepcopy(graph_FMI)\n",
    "\n",
    "    # Initialize weights at each node with zeros\n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node]['weights'] = np.zeros((graph.nodes[node]['X_train'].shape[1], 1))\n",
    "\n",
    "    # Federated gradient descent\n",
    "    prev_avg_delta_w = 1001\n",
    "    curr_avg_delta_w = 1000\n",
    "    i = 0\n",
    "    while i < max_iter and prev_avg_delta_w - curr_avg_delta_w > tol:\n",
    "        # Compute updated weights for all nodes\n",
    "        delta_w = {}\n",
    "        updates = {}\n",
    "        for node in graph.nodes:\n",
    "            # Extract data for the current node\n",
    "            X_train, y_train = graph.nodes[node]['X_train'], graph.nodes[node]['y_train']\n",
    "            X_test, user_signals = graph.nodes[node]['X_test'], graph.nodes[node]['u']\n",
    "            w_current = graph.nodes[node]['weights']\n",
    "\n",
    "            # Term (I): Training error gradient\n",
    "            term_1 = (2 / len(y_train)) * X_train.T.dot(X_train.dot(w_current) - y_train)\n",
    "\n",
    "            ### TASK ###\n",
    "            # Term (II): Subjective explainability gradient\n",
    "            # term_2 = \n",
    "\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Term (III): Graph regularization gradient\n",
    "            term_3 = 2 * alpha * sum(\n",
    "                w_current - graph.nodes[neighbor]['weights'] for neighbor in graph.neighbors(node)\n",
    "            )\n",
    "\n",
    "            # Update rule\n",
    "            updates[node] = w_current - l_rate * (term_1 + term_2 + term_3)\n",
    "\n",
    "        # Apply updates synchronously\n",
    "        for node, new_weights in updates.items():\n",
    "            delta_w[node] = np.linalg.norm(graph.nodes[node]['weights'] - new_weights)\n",
    "            graph.nodes[node]['weights'] = new_weights\n",
    "\n",
    "        # Calculate the average of weight differences\n",
    "        prev_avg_delta_w = curr_avg_delta_w\n",
    "        curr_avg_delta_w = np.array(list(delta_w.values())).mean()\n",
    "        if logging:\n",
    "            print(f\"Iteration #{i}: avg_delta_w = {curr_avg_delta_w}\")\n",
    "\n",
    "        # Iteration step\n",
    "        i += 1\n",
    "\n",
    "    # Logging\n",
    "    if i == max_iter:\n",
    "        print(f\"Maximum iteration reached\\n\")\n",
    "    else:\n",
    "        print(f\"Convergence reached at iteration #{i}\\n\")\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2e409-351f-45b7-a2c3-ae2d03d7f109",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "125f82ce9310776d2068a7d11019da5a",
     "grade": true,
     "grade_id": "cell-9dc2655f3c975cee",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform Federated Gradient Descent with Subjective Explainability term \n",
    "# with default parameters\n",
    "graph_trained_Exp = FedGD_Exp(G_FMI, \n",
    "                              alpha=0.5, \n",
    "                              rho=0.5, \n",
    "                              l_rate=0.05, \n",
    "                              max_iter=1000, \n",
    "                              tol=0.0001, \n",
    "                              logging=False)\n",
    "\n",
    "# Extract the updated local models' parameters\n",
    "weights_data_Exp = np.array([graph_trained_Exp.nodes[node]['weights'] for node in graph_trained_Exp.nodes])\n",
    "\n",
    "# Perform Federated Gradient Descent with default parameters\n",
    "graph_trained = FedGD(G_FMI, \n",
    "                      alpha=0.5, \n",
    "                      l_rate=0.05, \n",
    "                      max_iter=1000, \n",
    "                      tol=0.0001, \n",
    "                      logging=False)\n",
    "\n",
    "# Extract the updated local models' parameters\n",
    "weights_data = np.array([graph_trained.nodes[node]['weights'] for node in graph_trained.nodes])\n",
    "\n",
    "print(f\"Basic FedGD: the final weights for the node #0 is\\n{weights_data[0]}\\n\")\n",
    "print(f\"FedGD with Subjective Explainability term: the final weights for the node #0 is\\n{weights_data_Exp[0]}\\n\")\n",
    "\n",
    "# Sanity checks\n",
    "\n",
    "# The output of FedGD_Exp must be different from FedGD\n",
    "assert np.all(weights_data != weights_data_Exp), \"FedGD and FedGD_Exp must result in different model parameters!\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee5f0df-3c20-48fc-8918-373e6e565fe9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "189a8fa5ad4012a9e1553d9cea1d86fd",
     "grade": false,
     "grade_id": "cell-1143f0c7d57c3433",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## FedGD with Zero-Loss Nodes\n",
    "\n",
    "A malicious node within an empirical graph can exploit the training process by setting its local loss function to zero, thereby replicating the model weights of its neighboring nodes. This adversarial strategy enables the node to acquire sensitive information embedded within these weights, potentially compromising the privacy of the system.\n",
    "\n",
    "In the following task, we simulate the described attack by designating two nodes: Alice as the victim and Bob as the attacker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b6c41-6a51-4db4-9a08-8c74bf33bf47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83003f528e75830f729e0350603f4abf",
     "grade": false,
     "grade_id": "cell-944c74834b58fe15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Task 6.2 - Privacy Attack- Where is Alice?\n",
    "\n",
    "__Task description:__ \n",
    "1. Analyze the `FedGD_zero_loss` function and apply it to the `G_Alice_Bob` empirical graph.\n",
    "2. Determine the appropriate input parameter values that enable Bob to replicate Alice's local model parameters.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8cf30-d507-4c4a-8662-8080f4e586a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb88a430d9c697536d4fe93c4823e94b",
     "grade": false,
     "grade_id": "cell-ce881cf45d9913d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "For simplicity, we redefine the features and labels for the data points as follows:\n",
    "\n",
    "* __Features:__ The number of days since December 1st.\n",
    "* __Label:__ The average daily temperature.\n",
    "\n",
    "Furthermore, we apply `StandardScaler()` to standardize both features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eded86b-e634-46e4-a9c0-fbe160ded368",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "023d5a6677fea0477aa3ea70e72804af",
     "grade": false,
     "grade_id": "cell-b7fd3e7b05c77b34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the reference date as 1st December of the same year\n",
    "reference_date = pd.Timestamp(year=2023, month=12, day=1)\n",
    "\n",
    "# Calculate days elapsed since 01-12\n",
    "data[\"winter_day\"] = (data[\"Timestamp\"] - reference_date).dt.days\n",
    "\n",
    "# Aggregate the data by day and name\n",
    "daily_avg_temp = data.groupby([\"winter_day\", \"name\"]).agg(\n",
    "    avg_temp=(\"temp\", \"mean\"),\n",
    "    Latitude=(\"Latitude\", \"first\"),\n",
    "    Longitude=(\"Longitude\", \"first\"),\n",
    ").reset_index()\n",
    "\n",
    "# Standardize 'winter_day' and 'avg_temp'\n",
    "scaler_winter_day = StandardScaler()\n",
    "scaler_avg_temp = StandardScaler()\n",
    "daily_avg_temp['winter_day_scaled'] = scaler_winter_day.fit_transform(daily_avg_temp['winter_day'].values.reshape(-1, 1))\n",
    "daily_avg_temp['avg_temp_scaled'] = scaler_avg_temp.fit_transform(daily_avg_temp['avg_temp'].values.reshape(-1, 1))\n",
    "\n",
    "print(f\"The data point example:\")\n",
    "print(daily_avg_temp[daily_avg_temp['name'] == 'Alajärvi Möksy'])\n",
    "\n",
    "G_Alice_Bob = copy.deepcopy(G_FMI_no_edges)\n",
    "for node, attrs in G_Alice_Bob.nodes(data=True):\n",
    "    station = attrs['name']\n",
    "    station_data = daily_avg_temp[daily_avg_temp['name'] == station]\n",
    "    X_train = station_data['winter_day_scaled'].values.reshape(-1, 1)\n",
    "    y_train = station_data['avg_temp_scaled'].values.reshape(-1, 1)\n",
    "    G_Alice_Bob.nodes[node]['X_train'] = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    G_Alice_Bob.nodes[node]['y_train'] = y_train\n",
    "    \n",
    "    # Remove 'X_test' and 'y_test' if they exist\n",
    "    G_Alice_Bob.nodes[node].pop('X_test', None)\n",
    "    G_Alice_Bob.nodes[node].pop('y_test', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf30a6c-d745-4879-a07e-de4bcb98c7eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbcf874f1930d62a75767169effad1a2",
     "grade": false,
     "grade_id": "cell-3bd8ee8376ab47a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FedGD_zero_loss(graph_FMI, \n",
    "                    zero_loss_nodes=[], \n",
    "                    alpha=0.5, \n",
    "                    l_rate=0.1,\n",
    "                    max_iter=1000, \n",
    "                    tol=0.0001, \n",
    "                    logging=False, \n",
    "                    n_init=5):\n",
    "    \"\"\"\n",
    "    Performs Federated Gradient Descent (FedGD) with multiple random initializations,\n",
    "    enforcing zero loss on specified nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_FMI : networkx.Graph\n",
    "        A graph where each node represents a station, containing:\n",
    "        - 'X_train' (numpy.ndarray): Feature matrix for local training.\n",
    "        - 'y_train' (numpy.ndarray): Target values for training.\n",
    "        - 'weights' (numpy.ndarray): Model parameters, initialized randomly.\n",
    "\n",
    "    zero_loss_nodes : list, optional (default=[])\n",
    "        List of node IDs where the loss term is ignored during updates, \n",
    "        enforcing zero loss for those nodes.\n",
    "\n",
    "    alpha : float, optional (default=0.5)\n",
    "        Regularization parameter controlling the influence of neighboring nodes.\n",
    "\n",
    "    l_rate : float, optional (default=0.1)\n",
    "        Learning rate for the gradient descent updates.\n",
    "\n",
    "    max_iter : int, optional (default=1000)\n",
    "        Maximum number of iterations for gradient descent.\n",
    "\n",
    "    tol : float, optional (default=0.0001)\n",
    "        Convergence threshold based on the average weight change.\n",
    "\n",
    "    logging : bool, optional (default=False)\n",
    "        If True, prints the average weight change and iteration progress.\n",
    "\n",
    "    n_init : int, optional (default=5)\n",
    "        Number of random initializations to find the best-performing model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_graph : networkx.Graph\n",
    "        A modified copy of `graph_FMI` where each node's 'weights' attribute \n",
    "        contains the trained model parameters, optimized over multiple initializations.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The algorithm runs `n_init` times with different random initializations, selecting \n",
    "      the model with the lowest average training loss.\n",
    "    - The weight update rule consists of:\n",
    "        (I) Training error gradient (for regular nodes).\n",
    "        (II) Graph regularization gradient (neighboring influence).\n",
    "    - Nodes in `zero_loss_nodes` update weights only using regularization.\n",
    "    - The stopping criterion is based on weight change convergence.\n",
    "    - If `max_iter` is reached, a message is printed indicating termination.\n",
    "    \"\"\"\n",
    "    best_graph = None\n",
    "    best_avg_loss = float('inf')  # We will use the lowest average loss over nodes as the criteria\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for init_iter in range(n_init):\n",
    "        print(f\"Running initialization {init_iter + 1}/{n_init}\")\n",
    "        \n",
    "        # Deep copy the input graph to reset the weights for each initialization\n",
    "        graph = copy.deepcopy(graph_FMI)\n",
    "        \n",
    "        # Initialize weights at each node (random initialization)\n",
    "        for node in graph.nodes:\n",
    "            # Initialize weights\n",
    "            graph.nodes[node]['weights'] = np.random.uniform(-1, 1, (graph.nodes[node]['X_train'].shape[1], 1))\n",
    "\n",
    "        # Federated gradient descent\n",
    "        prev_avg_delta_w = 1001\n",
    "        curr_avg_delta_w = 1000\n",
    "        i = 0\n",
    "        while i < max_iter and prev_avg_delta_w - curr_avg_delta_w > tol:\n",
    "            # Compute updated weights for all nodes\n",
    "            delta_w = {}\n",
    "            updates = {}\n",
    "            for node in graph.nodes:\n",
    "                X_train, y_train = graph.nodes[node]['X_train'], graph.nodes[node]['y_train']\n",
    "                w_current = graph.nodes[node]['weights']\n",
    "                term_1 = (2 / len(y_train)) * X_train.T.dot(X_train.dot(w_current) - y_train)\n",
    "                term_2 = 2 * alpha * sum(\n",
    "                    w_current - graph.nodes[neighbor]['weights'] for neighbor in graph.neighbors(node)\n",
    "                )\n",
    "                # Zero loss for the defined nodes\n",
    "                if node in zero_loss_nodes:\n",
    "                    updates[node] = w_current - l_rate * term_2\n",
    "                else:\n",
    "                    updates[node] = w_current - l_rate * (term_1 + term_2)\n",
    "\n",
    "            # Apply updates synchronously\n",
    "            for node, new_weights in updates.items():\n",
    "                delta_w[node] = np.linalg.norm(graph.nodes[node]['weights'] - new_weights)\n",
    "                graph.nodes[node]['weights'] = new_weights\n",
    "                \n",
    "            # Calculate the average of weight differences\n",
    "            prev_avg_delta_w = curr_avg_delta_w\n",
    "            curr_avg_delta_w = np.array(list(delta_w.values())).mean()\n",
    "            if logging:\n",
    "                print(f\"Iteration #{i}: avg_delta_w = {curr_avg_delta_w}\")\n",
    "\n",
    "            # Iteration step\n",
    "            i += 1\n",
    "\n",
    "        # Check if this initialization produced the best result\n",
    "        avg_loss = 0\n",
    "        for node, attr in graph.nodes(data=True):\n",
    "            local_loss = mean_squared_error(attr['y_train'], attr['X_train'].dot(attr['weights']))\n",
    "            avg_loss += local_loss\n",
    "        avg_loss /= len(graph.nodes)\n",
    "        \n",
    "        if avg_loss < best_avg_loss:\n",
    "            print(f\"The best loss is updated: {avg_loss}\")\n",
    "            best_avg_loss = avg_loss\n",
    "            best_graph = graph\n",
    "            \n",
    "        # Logging\n",
    "        if i == max_iter:\n",
    "            print(f\"Initialization {init_iter + 1}: Maximum iteration reached\\n\")\n",
    "        else:\n",
    "            print(f\"Initialization {init_iter + 1}: Convergence reached at iteration #{i}\\n\")\n",
    "\n",
    "    # Return the best result after testing all initializations\n",
    "    return best_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f111d5-da00-48f3-82ba-d81f2166a111",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a5e5cb886796041169f58f2463432a2",
     "grade": false,
     "grade_id": "cell-19221ae6ff5880e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4df6bd-a0bd-4098-8bd7-95814b4e0070",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bd7b8e83ee0bf82ade8f6c479933933",
     "grade": false,
     "grade_id": "cell-408a1e74992d45d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the node Alice at location Vantaa Helsinki-Vantaan lentoasema\n",
    "Alice_node_id = 0\n",
    "for node_id, attrs in G_Alice_Bob.nodes(data=True):\n",
    "    if attrs['name'] == \"Vantaa Helsinki-Vantaan lentoasema\":\n",
    "        Alice_node_id = node_id\n",
    "        G_Alice_Bob.nodes[node_id]['user_name'] = \"Alice\"\n",
    "        break\n",
    "        \n",
    "print(f\"Alice is located at node #{Alice_node_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dfa7d7-6b75-4372-b483-fc1c6127d4d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9bdb18fe28d7b732e0d503b6215ceaf",
     "grade": false,
     "grade_id": "cell-04e1320c94336b8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee877a14-ee3f-4246-81c0-5de8de65edd6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e82d506783148c859f26daa9843c9ae",
     "grade": false,
     "grade_id": "cell-6d80f7b121dc7825",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the node Bob at location Oulu lentoasema\n",
    "Bob_node_id = 0\n",
    "for node_id, attrs in G_Alice_Bob.nodes(data=True):\n",
    "    if attrs['name'] == \"Oulu lentoasema\":\n",
    "        Bob_node_id = node_id\n",
    "        G_Alice_Bob.nodes[node_id]['user_name'] = \"Bob\"\n",
    "        break\n",
    "        \n",
    "print(f\"Bob is located at node #{Bob_node_id}\")\n",
    "\n",
    "# Add an edge between Bob's and Alice's nodes\n",
    "G_Alice_Bob.add_edge(Alice_node_id, Bob_node_id, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248ea23-bbb2-46b2-9a57-d617588e9fcb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3994d8a3e7e34fc364239f4b99197a17",
     "grade": false,
     "grade_id": "cell-7e7695351e5ee619",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotFMI(G_Alice_Bob, show_IDs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc1aca-24bf-4271-81ae-43dbd45db560",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2e1a8fe39906c013ee921d058786575",
     "grade": false,
     "grade_id": "cell-9775e03aca9f57f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Train the local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7dc74-bb4a-4f0a-a241-2309a746493b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17ba02123734eb8248668c9cc69065b2",
     "grade": false,
     "grade_id": "cell-f128f86a67e0cbe7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### TASK ###\n",
    "# Choose the attributes and apply the Federated Gradient Descent alogorithm \n",
    "# to the G_Alice_Bob, so Bob replicates Alice's local model parameters.\n",
    "# IMPORTANT: all solutions that replicate the Alice's weights without applying FedGD_zero_loss(...)\n",
    "#            will receive 0 points!!!\n",
    "\n",
    "# G_Alice_Bob_trained = FedGD_zero_loss(...)\n",
    " \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"{G_Alice_Bob_trained.nodes[Alice_node_id]['user_name']} is located at {G_Alice_Bob_trained.nodes[Alice_node_id]['name']}\")\n",
    "print(f\"The local model parameters are\\n{G_Alice_Bob_trained.nodes[Alice_node_id]['weights']}\\n\")\n",
    "print(f\"{G_Alice_Bob_trained.nodes[Bob_node_id]['user_name']} is located at {G_Alice_Bob_trained.nodes[Bob_node_id]['name']}\")\n",
    "print(f\"The local model parameters are\\n{G_Alice_Bob_trained.nodes[Bob_node_id]['weights']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917d4cc-3b87-4801-b23f-c55750c62b13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d9668b541d934dec4eacc86c2a8e7c3",
     "grade": true,
     "grade_id": "cell-f7f4f666ce745886",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the chosen answer option is adequate\n",
    "w_Alice = G_Alice_Bob_trained.nodes[Alice_node_id]['weights']\n",
    "w_Bob = G_Alice_Bob_trained.nodes[Bob_node_id]['weights']\n",
    "\n",
    "assert np.allclose(w_Alice, w_Bob, atol=0.03), \"Bob's weights must be similar to Alice's ones!\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cc0ce-ba49-464b-bfcd-2fa1e0b6fe8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba95664c5d914e2ab3431afa4cf6c503",
     "grade": false,
     "grade_id": "cell-aeb1b10bf04814b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract weights, features, and labels\n",
    "w_Alice = G_Alice_Bob_trained.nodes[Alice_node_id]['weights']\n",
    "X_Alice = G_Alice_Bob_trained.nodes[Alice_node_id]['X_train'][:, 0].reshape(-1, 1)\n",
    "y_Alice = G_Alice_Bob_trained.nodes[Alice_node_id]['y_train']\n",
    "\n",
    "# Plot actual data points\n",
    "plt.scatter(X_Alice, y_Alice, c='b', label='True Data')\n",
    "\n",
    "# Plot predictions from the trained model\n",
    "plt.scatter(X_Alice, X_Alice.dot(w_Alice[0, :]) + w_Alice[1, :], c='r', label='Predicted Data')\n",
    "\n",
    "# Add title and legend\n",
    "plt.title(\"Linear Model Trained at Alice's Node\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeff4f8-c091-4b78-bb1e-97004451f78c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bdbcd3b6031462b80ebef7221d1fe35",
     "grade": false,
     "grade_id": "cell-a4f9dae28eabea8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Find the Alice's location\n",
    "\n",
    "Now that Bob's local weights replicate those of Alice, he can systematically examine all stations, compare the model weights, and infer Alice's location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018da4a-42a2-4ad9-a7eb-a79beea75443",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55246a11a90de3162a0a8d90a4a29e97",
     "grade": true,
     "grade_id": "cell-23c884f2e2c8e603",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_nearest = 2\n",
    "w_Bob = G_Alice_Bob_trained.nodes[Bob_node_id]['weights']\n",
    "\n",
    "delta_w = {}\n",
    "for node_id, attrs in G_Alice_Bob_trained.nodes(data=True):\n",
    "    w_local = attrs['weights']\n",
    "    delta_w[node_id] = mean_squared_error(w_local, w_Bob)\n",
    "        \n",
    "# Get the n_nearest nodes with the smallest values in delta_w\n",
    "n_nearest_nodes = sorted(delta_w.items(), key=lambda x: x[1])[:n_nearest]\n",
    "\n",
    "# Extract only the node IDs (or keep the node ID-value pairs if needed)\n",
    "n_nearest_node_ids = [node_id for node_id, _ in n_nearest_nodes]\n",
    "\n",
    "# Print the results\n",
    "print(f\"The {n_nearest} nearest nodes based on delta_w:\")\n",
    "print(n_nearest_node_ids)\n",
    "\n",
    "# Sanity check\n",
    "assert set(n_nearest_node_ids) == set([Alice_node_id, Bob_node_id])\n",
    "print(\"Sanity check passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97091b27-26a0-4033-a25b-caec2d480076",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46a9e4274b23c451cde796496d57ca28",
     "grade": false,
     "grade_id": "cell-e3fd65ab375c0dcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotFMI(G_Alice_Bob_trained, show_IDs=False, nodes_red=n_nearest_node_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93577b50-5930-423c-a655-d0a5f966454c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7532026764cc1dc82a352b947d4a944",
     "grade": false,
     "grade_id": "cell-6306d887a54bbce3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Question 6.1 - AI key requirements\n",
    "\n",
    "__Formulation:__\n",
    "    \n",
    "The European Commission set up the High-Level Expert Group on Artificial Intelligence (AI HLEG) in 2018. What requirement for trustworthy AI was not put forward? \n",
    "\n",
    "__Note:__ \n",
    "    \n",
    "The requirements are rephrased. Pay attention to the meaning, not wording.\n",
    "\n",
    "__Answer Options:__\n",
    "\n",
    "1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.\n",
    "2.  Accountability must be ensured for AI systems and their outcomes, both before and after their development, deployment and use.\n",
    "3. Prevention of harm to privacy and adequate data governance.\n",
    "4. AI systems must reliably behave as intended while minimizing unintentional and unexpected harm, and preventing unacceptable harm.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68e8c4-f3c4-4bf2-8a6f-78be8b17fd5c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d5233f9cc279be3b334ee3f004503aa",
     "grade": false,
     "grade_id": "cell-b1ef6bdc91f1c1b3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Question ###\n",
    "\n",
    "# Assign the variable to the answer option from the list above\n",
    "# answer =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090eacfb-e2fa-4635-98fb-0a8fc169dd08",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fa8beb14f302a73da26e888ce418549",
     "grade": true,
     "grade_id": "cell-dbfc16d80bcd16e9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the chosen answer option is adequate\n",
    "assert answer in [1, 2, 3, 4], \"Choose the answer option from the provided list.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226eefc2-4217-4e0f-9aa9-1cd2dcfb8305",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9eab7bdea7251d4d6a4bd359c3c9e39e",
     "grade": false,
     "grade_id": "cell-1959131cc348c489",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Question 6.2 - Explainability\n",
    "\n",
    "__Formulation:__\n",
    "    \n",
    "What does the explainability of a trained personalized model imply?\n",
    "\n",
    "__Answer Options:__\n",
    "\n",
    "1. The model must replicate the acknowledged formulas.\n",
    "2. A human can replicate the model and perform the prediction delivery process.\n",
    "3. A human can guess the prediction delivered by the model with a certain level of accuracy. \n",
    "4. The model structure must be available to public.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71337598-f525-47b5-bf33-c8c7e45fd1d6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60e562deffbf7a2296924ece1f26f551",
     "grade": false,
     "grade_id": "cell-0140038cbdf860da",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Question ###\n",
    "\n",
    "# Assign the variable to the answer option from the list above\n",
    "# answer =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2765b8-dc71-4bf7-931b-6366f2d779b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3850bada584a5a03be05cad57ba5da4",
     "grade": true,
     "grade_id": "cell-41007893526d2676",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the chosen answer option is adequate\n",
    "assert answer in [1, 2, 3, 4], \"Choose the answer option from the provided list.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
